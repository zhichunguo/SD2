{"titles": ["Using MPI: portable parallel programming with the message-passing interface", "MPI--the Complete Reference: the MPI core", "PETSc users manual", "A high-performance, portable implementation of the MPI message passing interface standard", "Domain decomposition: parallel multilevel methods for elliptic partial differential equations", "Efficient management of parallelism in object-oriented numerical software libraries", "PETSc web page", "The international exascale software project roadmap", "PETSc web page, 2001", "Optimization of collective communication operations in MPICH", "Sourcebook of parallel computing", "Data sieving and collective I/O in ROMIO", "On implementing MPI-IO portably and with high performance", "CFD vision 2030 study: a path to revolutionary computational aerosciences", "PETSc 2.0 users manual", "Toward exascale resilience", "User\u2019s Guide for mpich, a Portable Implementation of MPI", "Parallel netCDF: A high-performance scientific I/O interface", "An adaptive performance modeling tool for GPU architectures", "Toward exascale resilience: 2014 update", "PETSc home page", "MPI: A message-passing interface standard", "Toward scalable performance visualization with Jumpshot", "A comparison of domain decomposition techniques for elliptic partial differential equations and their parallel implementation", "Multiphysics simulations: Challenges and opportunities", "Improving the performance of collective operations in MPICH", "Reproducible measurements of MPI performance characteristics", "MPICH2: A new start for MPI implementations", "An introduction to domain decomposition methods: algorithms, theory, and parallel implementation", "PETSc users manual revision 3.8", "Optimization Environments and the", "PVM: A Users' Guide and Tutorial for Network Parallel Computing", "Fault tolerance in message passing interface programs", "An abstract-device interface for implementing portable parallel-I/O interfaces", "Numerical simulation of vortex dynamics in type-II superconductors", "Modern software tools for scientific computing", "Users guide for ROMIO: A high-performance, portable MPI-IO implementation", "MPI-2: Extending the message-passing interface", "A parallel version of the fast multipole method", "Exploiting hierarchy in parallel computer networks to optimize collective operation performance", "High-performance parallel implicit CFD", "Parallel I/O prefetching using MPI file caching and I/O signatures", "Parallel Newton--Krylov--Schwarz algorithms for the transonic full potential equation", "Method and apparatus for real-time parallel delivery of segments of a large payload file", "Parallel programming using C++", "PETSc users manual (Tech. Rep. ANL-95/11-Revision 3.7)", "Design and evaluation of Nemesis, a scalable, low-latency, message-passing communication subsystem", "Achieving high sustained performance in an unstructured mesh CFD application", "Wide-area implementation of the message passing interface", "MPI on a Million Processors", "Optimizing noncontiguous accesses in MPI\u2013IO", "Design and Implementation of MPICH2 over InfiniBand with RDMA Support", "Toward realistic performance bounds for implicit CFD codes", "Using advanced MPI: Modern features of the message-passing interface", "MPI-2: Advanced Features of the Message Passing Interface", "Newton-Krylov-Schwarz methods in CFD", "From trace generation to visualization: A performance framework for distributed parallel systems", "Using mpi-2", "A case for using MPI's derived datatypes to improve I/O performance", "Globalized Newton-Krylov-Schwarz algorithms and software for parallel implicit CFD", "Noncontiguous i/o through pvfs", "Applications-driven parallel I/O", "High performance MPI-2 one-sided communication over InfiniBand", "PETSc Users Manual, Argonne National Laboratory", "A multiplatform study of I/O behavior on petascale supercomputers", "MPI+ MPI: a new hybrid approach to parallel programming with MPI plus shared memory", "Implementation and evaluation of shared-memory communication and synchronization operations in MPICH2 using the Nemesis communication subsystem", "Noncontiguous i/o accesses through mpi-io", "Modeling the performance of an algebraic multigrid cycle on HPC platforms", "An efficient format for nearly constant-time access to arbitrary time intervals in large trace files", "PETSc", "Performance modeling for systematic performance tuning", "Design and implementation of message-passing services for the Blue Gene/L supercomputer", "A comparison of some domain decomposition and ILU preconditioned iterative methods for nonsymmetric elliptic problems", "Hiding I/O latency with pre-execution prefetching for parallel applications", "Users' Guide to mpich, a Portable Implementation of MPI", "Remote memory access programming in MPI-3", "Avoiding hot-spots on two-level direct networks", "Performance modeling and tuning of an unstructured mesh CFD application", "High performance file I/O for the Blue Gene/L supercomputer", "Parallel computing and domain decomposition", "Multi-core and network aware MPI topology functions", "Chameleon parallel programming tools users manual", "Formal analysis of MPI-based parallel programs", "MPI on millions of cores", "A high-performance MPI implementation on a shared-memory vector supercomputer", "PMI: A scalable parallel process-management interface for extreme-scale systems", "Collective communication on architectures that support simultaneous communication over multiple links", "Optimizing the synchronization operations in message passing interface one-sided communication", "User\u2019s Guide for mpich, a Portable Implementation of MPI. Mathematics and Computer Science Division, Argonne National Laboratory, 1996", "Fine-grained multithreading support for hybrid threaded MPI programming", "Data transfers between processes in an SMP system: Performance study and application to MPI", "The parallel scalability of the spectral transform method", "Implementing fast and reusable datatype processing", "Learning from the Success of MPI", "MPICH Working Note: Creating a new MPICH device using the Channel interface DRAFT", "Scalable, extensible, and portable numerical libraries", "Implementation and shared-memory evaluation of MPICH2 over the Nemesis communication subsystem", "A standard interface for debugger access to message queue information in MPI", "Domain decomposition on parallel computers", "A scalable process-management environment for parallel programs", "Thread-safety in an MPI implementation: Requirements and analysis", "A test of moving mesh refinement for 2-D scalar hyperbolic problems", "Using MPI: portable parallel programming with the message-passing interface. Scientific and engineering computation", "Complexity of parallel implementation of domain decomposition techniques for elliptic partial differential equations", "MPICH-GQ: Quality-of-service for message passing programs", "MPICH2 User\u2019s Guide Version 1.0. 7 Mathematics and Computer Science Division Argonne National Laboratory", "Test suite for evaluating performance of MPI implementations that support MPI_THREAD_MULTIPLE", "Electron injection by a nanowire in the bubble regime", "PETSc Users Manual ANL-95/11-Revision 2.1. 5", "A. Skjellum. Using MPI: Portable Parallel Programming with the Message-Passing Interface", "Improving the performance of MPI derived datatypes", "Sowing MPICH: A case study in the dissemination of a portable environment for parallel scientific computing", "Domain decomposition methods for partial differential equations", "Dynamic process management in an MPI setting", "A taxonomy of programming models for symmetric multiprocessors and SMP clusters", "Domain decomposition methods in computational fluid dynamics", "Local uniform mesh refinement with moving grids", "Big data and extreme-scale computing: Pathways to convergence-toward a shaping strategy for a future software and data ecosystem for scientific inquiry", "Hierarchical collectives in MPICH2", "Tutorial on mpi: The message-passing interface", "The portable, extensible toolkit for scientific computation", "User\u2019s guide for mpe extensions for mpi programs", "An implementation and evaluation of the MPI 3.0 one\u2010sided communication interface", "The Blue Waters super-system for super-science", "MPI at Exascale", "Test suite for evaluating performance of multithreaded MPI communication", "Programming for exascale computers", "Issues in developing a thread-safe MPI implementation", "Amphotericin B, its production, and its salts", "Leveraging MPI\u2019s one-sided communication interface for shared-memory programming", "LACIO: A new collective I/O strategy for parallel I/O systems", "Goals guiding design: PVM and MPI", "MPI the Complete Reference: The MPI-2 Extensions, Vol. 2", "Whirlpool PLAs: a regular logic structure and their synthesis", "PETSc, the portable, extensible toolkit for scientific computation", "Installation guide for mpich, a portable implementation of MPI", "Domain decomposition with local mesh refinement", "A comparison of some domain decomposition algorithms for nonsymmetric elliptic problems", "Toward efficient support for multithreaded MPI communication", "Components and interfaces of a process management system for parallel programs", "Enabling concurrent multithreaded MPI communication on multicore petascale systems", "Implementing MPI-IO atomic mode without file system support", "Why are PVM and MPI so different?", "PETSc home page, 2001", "An abstract device definition to support the implementation of a high-level point-to-point message-passing interface", "MPICH User\u2019s Guide", "Experiences with Domain Decomposition", "Faster topology-aware collective algorithms through non-minimal communication", "Practical model-checking method for verifying correctness of MPI programs", "Efficient implementation of MPI-2 passive one-sided communication on InfiniBand clusters", "Parallel netCDF: A scientific high-performance I/O interface", "An experimental evaluation of the parallel I/O systems of the IBM SP and Intel Paragon using a production application", "A model implementation of MPI", "Simplified linear equation solvers users manual", "Krylov methods preconditioned with incompletely factored matrices on the CM-2", "Toward performance models of MPI implementations for understanding application scaling issues", "An introductory exascale feasibility study for FFTs and multigrid", "Formal verification of programs that use MPI one-sided communication", "Minimizing synchronization overhead in the implementation of MPI one-sided communication", "I/O in parallel applications: The weakest link", "The MPI communication library: its design and a portable implementation", "Performance analysis of the lattice Boltzmann model beyond Navier-Stokes", "Implementing MPI on the BlueGene/L supercomputer", "MPI on BlueGene/L: Designing an efficient general purpose messaging solution for a large cellular system", "Volume integral equations in non\u2010linear 3\u2010D magnetostatics", "Using MPI: Portable Parallel Programming with the Message Passing Interface. 1999", "Users manual for the Chameleon parallel programming tools", "Natively supporting true one-sided communication in MPI on multi-core systems with InfiniBand", "Implementing efficient dynamic formal verification methods for MPI programs", "An evaluation of implementation options for MPI one-sided communication", "Using MPI: Portable Parallel Programming with the Message Passing", "Hybrid static/dynamic scheduling for already optimized dense matrix factorization", "Self-consistent MPI performance guidelines", "A., Skjellum", "Solution of dense systems of linear equations arising from integral-equation formulations", "Annotations for productivity and performance portability", "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface", "Installation and user\u2019s guide for mpich, a portable implementation of MPI", "MPICH ADI implementation reference manual", "Parallel multilevel methods for elliptic partial differential equations", "Petsc 2.0 users manual", "Applied mathematics at the US Department of Energy: Past, present and a view to the future", "Collective error detection for MPI collective operations", "Scalable Unix tools on parallel processors", "Revealing the performance of MPI RMA implementations", "Scalable Unix commands for parallel processors: A high-performance implementation", "R\u00e9sistance g\u00e9n\u00e9tique des petits ruminants aux helminthes en Afrique", "I/O characterization of a portable astrophysics application on the IBM SP and Intel Paragon", "Convergence rate estimate for a domain decomposition method", "Performance expectations and guidelines for MPI derived datatypes", "Investigating high performance RMA interfaces for the MPI-3 standard", "Toward message passing for a million processes: Characterizing MPI on a massive scale Blue Gene/P", "Communication analysis of parallel 3D FFT for flat cartesian meshes on large Blue Gene systems", "A formal approach to detect functionally irrelevant barriers in MPI programs", "Using MPI: Portable Parallel Programming with the Message Passing Iinterface", "Towards millions of communicating threads", "Reducing parallel communication in algebraic multigrid through sparsification", "Modeling the performance of an algebraic multigrid cycle using hybrid mpi/openmp", "Architectural constraints to attain 1 exaflop/s for three scientific application classes", "Exploring parallel I/O concurrency with speculative prefetching", "Predicting memory-access cost based on data-access patterns", "Using MPI-2 Advanced Features of the Message Passing Interface, ser. Scientific and Engineering Computation", "MPI: The Complete Reference. The MPI-2 Extensions", "MPICH working note: The second-generation ADI for the MPICH implementation of MPI", "The design of data-structure-neutral libraries for the iterative solution of sparse linear systems", "User\u2019s Guide for mpich, a Portable Implementation of MPI Version 1.2. 1", "An initial implementation of MPI", "A multilevel approach to topology-aware collective operations in computational grids", "Runtime checking of datatype signatures in MPI", "A test implementation of the MPI draft message-passing standard", "Computational fluid dynamics on parallel processors", "Non-data-communication overheads in MPI: analysis on Blue Gene/P", "User\u2019s Guide for MPE: Extensions for MPI Programs", "A Viewpoint on the Quantity\" Plane Angle\"", "Modeling MPI communication performance on SMP nodes: Is it time to retire the ping pong test", "MPI 3 and beyond: why MPI is successful and what challenges it faces", "Scalable memory use in MPI: A case study with MPICH2", "Jumpshot-4 Users Guide", "Using MPI: portable parallel programming with the message-passing interface", "Exascale research: preparing for the post-Moore era", "Scalable Log Files for Parallel Program Trace Data DRAFT", "PVM and MPI are completely different", "Open issues in MPI implementation", "MPICH2 User\u2019s Guide version 1.0. 3", "Parallel implicit PDE computations: Algorithms and software", "Using MPI: Portable parallel processing with the Message Passing Interface", "Using MP1", "MPI-eine Einf\u00fchrung: portable parallele Programmierung mit dem Message-Passing Interface", "MPI at Exascale: Challenges for Data Structures and Algorithms.", "Optimizing of collective communication operations in MPICH", "Optimization Environments and the NEOS Server. Approximation Theory and Optimization, MD Buhmann and A. Iserles, eds", "Users manual for KSP data-structure-neutral codes implementing Krylov space methods", "Exploring the feasibility of lossy compression for PDE simulations", "A scalable MPI_Comm_split algorithm for exascale computing", "Using MPI\u20142nd Edition: Portable Parallel Programming with the Message Passing Interface", "MPI--the Complete Reference: The MPI Extensions", "Users guide for the ANL IBM SP1", "Parallel implicit methods for aerodynamics", "Parallel performance of domain-decomposed preconditioned Krylov methods for PDEs with locally uniform refinement", "A portable method for finding user errors in the usage of MPI collective operations", "Automatic memory optimizations for improving MPI derived datatype performance", "Providing efficient I/O redundancy in MPI environments", "Performing Arts: the Economic Dilemma; a Study of Problems Common to Theater, Opera, Music, and Dance", "Scalable non-blocking preconditioned conjugate gradient methods", "MPICH user\u2019s guide", "Optimizing sparse data structures for matrix-vector multiply", "The importance of non-data-communication overheads in MPI", "Message passing and threads", "More. Optimization environments and the NEOS server", "Load balancing for regular meshes on SMPs with MPI", "Processing MPI datatypes outside MPI", "Domain decomposition techniques for the parallel solution of nonsymmetric systems of elliptic boundary value problems", "Update on libraries for blue waters", "Scalable Input/Output: achieving system balance", "A science-based case for large-scale simulation", "Latency, bandwidth, and concurrent issue limitations in high-performance CFD.", "Parallel solution of the three-dimensional time-dependent Ginzburg-Landau equation", "MPICH Working Note: The implementation of the second generation MPICH ADI", "MPICH working note: The second-generation ADI for the MPICH implementation of MPI", "Users' Guide for mpich, a Portable Implementation of MPI. 1996", "Programming models for parallel computing", "CFD vision 2030 study: A path to revolutionary computational aerosciences: NASA", "Analysis of topology-dependent MPI performance on Gemini networks", "Minimizing MPI resource contention in multithreaded multicore environments", "Early experiences with the IBM SP1 and the high-performance switch", "An abstract device definition to support the implementation of a high-level message-passing interface", "Local uniform mesh refinement for elliptic partial differential equations", "The blue waters super-system for super-science. contemporary hpc architectures, jeffery vetter editor", "A decoupled execution paradigm for data-intensive high-end computing", "A simple, pipelined algorithm for large, irregular all-gather problems", "Improving the performance of MPI collective communication on switched networks", "High performance wide area data transfers over high performance networks", "Domain decomposition techniques for nonsymmetric systems of elliptic boundary value problems: Examples from CFD", "Towards a more complete understanding of sdc propagation", "Runtime support for object-based message-driven parallel applications on heterogeneous clusters", "EcoG: A power-efficient GPU cluster architecture for scientific computing", "Self-consistent MPI-IO performance requirements and expectations", "Analyzing the impact of supporting out-of-order communication on in-order performance with iWARP", "Portable, extensible toolkit for scientific computation (PETSc)", "Prototype of AM3: active mapper and monitoring module for Myrinet environments", "NIC-based atomic operations on Myrinet/GM", "MPICH model MPI implementation reference manual", "Early applications in the message-passing interface (MPI)", "PETSc Web page, 2012b", "Sk jellum, A.(1994)", "A hybrid format for better performance of sparse matrix-vector multiplication on a GPU", "Enabling the environmentally clean air transportation of the future: a vision of computational fluid dynamics in 2030", "A case for optimistic coordination in hpc storage systems", "Preparing algebraic multigrid for exascale", "Formal methods applied to high\u2010performance computing software design: a case study of MPI one\u2010sided communication\u2010based locking", "Analytical performance prediction for evaluation and tuning of GPGPU applications", "Extending the MPI-2 generalized request interface", "Self-consistent MPI performance requirements", "Nonuniformly communicating noncontiguous data: A case study with petsc and mpi", "The CH3 design for a simple implementation of ADI-3 for MPICH-2 with a TCP-based implementation", "MPICH2 design document", "Modern Software Tools in Scientific Computing, chapter Efficient Management of Parallelism in Object-Oriented Numerical Software Libraries", "Skjellum. Using MPI", "MPICH Web page", "Toward asynchronous and MPI-interoperable active messages", "MPI (Message Passing Interface).", "A pipelined algorithm for large, irregular all-gather problems", "Advanced flow-control mechanisms for the sockets direct protocol over infiniband", "e Skjellum, A.(1996). Using MPI\u2013Portable Parallel Programming with the Message Passing Interface", "Evaluating structured I/O methods for parallel file systems", "Mpich abstract device interface version 3.3 reference manual", "NIC-based atomic remote memory operations in Myrinet/GM", "Exploiting existing software in libraries: Successes, failures, and reasons why", "MPI\u2013the complete reference (2-volume set)", "User Guide for ROMIO: A High Performance", "Tuning MPI programs for peak performance", "Implementing MPI: The 1994 MPI implementors' workshop", "Portable Programming with the Message-Passing Interface", "Using MPI: portable parallel programming", "Domain-Decomposable Preconditioners for Second-Order", "Grid generation for time dependent problems: Criteria and methods", "Amphotericin A and its salts", "Enabling real-time multi-messenger astrophysics discoveries with deep learning", "Using node information to implement mpi cartesian topologies", "DAME: A runtime-compiled engine for derived datatypes", "Locality-optimized mixed static/dynamic scheduling for improving load balancing on SMPs", "Adaptive strategy for one-sided communication in MPICH2", "MPICH", "Designing a common communication subsystem", "Issues in accurate and reliable use of parallel computing in numerical programs", "Using MPI: Portable Parallel Programming with the Message-Passing Interface, seconde \u00e9dition", "Performance driven programmimg models", "Tuning MPI applications for peak performance", "Users manual for doctext: Producing documentation from C source code", "Parallel domain decomposition and the solution of nonlinear systems of equations", "A Comparison of Domain Decomposition Techniques for Elliptic Partial Differential Equations and Their Parallel Implementation.", "MPICH Working Note: the implementation of the second generation ADI", "Towards a more fault resilient multigrid solver", "Systematic reduction of data movement in algebraic multigrid solvers", "Efficient communication across the Internet in wide-area MPI", "a Lusk", "MPI: The Complete Reference. Volume 2, The MPI-2 Extensions. Scientific and Engineering Computation", "Optimization Environments and the NEOS Server in MD Buhmann and A. Iserles (eds.), Approximation Theory and Optimization", "New generation framework for petroleum reservoir simulation", "Scalable libraries for solving systems of nonlinear equations and unconstrained minimization problems", "MPE graphics-scalable X11 graphics in MPI", "MPICH-a portable implementation of MPI", "Local uniform mesh refinement on vector and parallel processors", "Process of recovering nystatin", "Algebraic multigrid on a dragonfly network: First experiences on a Cray XC30", "Nonblocking epochs in MPI one-sided communication", "Efficient multithreaded context ID allocation in MPI", "Adaptive thread distributions for SpMV on a GPU", "Software for petascale computing systems", "Improving the performance of tensor matrix vector multiplication in cumulative reaction probability based quantum chemistry codes", "MPICH abstract device interface", "Ewing Lusk", "A User\u2019s View of OpenMP: The Good, The Bad and the Ugly", "MPI: The Complete Reference, 2nd", "Users guide for the ANL IBM SP-1 draft,\"", "Implicit domain decomposition algorithms for steady, compressible aerodynamics", "Solving pdes on loosely-coupled parallel processors", "PETSc Users Manual, 2008", "Locus: a system and a language for program optimization", "Non-blocking preconditioned conjugate gradient methods for extreme-scale computing", "MPI resources online", "Performance modeling of algebraic multigrid on blue Gene/Q: lessons learned", "Understanding the requirements imposed by programming model middleware on a common communication subsystem.", "MPICH2 Abstract Device Interface Version 3.4 Reference Manual: Draft of May 20", "An evaluation of object-based data transfers on high performance networks", "Performance visualization for parallel programs", "Using MPI-2 Advanced Features of the Message-Passing Interface", "Skjellum (1999). Using MPI", "The Complete Reference, Vol. 2, The MPI Extensions", "Wide-area implementation of the message passing standard", "User\u2019s Guide for MPICH", "The MPI message-passing interface standard: overview and status", "Users manual for bfort: Producing Fortran interfaces to C source code", "The Scalable I/O Initiative", "Developing applications for a heterogeneous computing environment", "Recursive mesh refinement on hypercubes", "Local uniform mesh refinement on loosely-coupled parallel processors", "Portable, Extensible Toolkit for Scientific Computation (PETSc)", "Node aware sparse matrix\u2013vector multiplication", "Improving performance models for irregular point-to-point communication", "Rethinking key\u2013value store for parallel i/o optimization", "Tapspmv: Topology-aware parallel sparse matrix vector multiplication", "Applications of the streamed storage format for sparse matrix operations", "Technical Report 2014-218178", "MPI-interoperable generalized active messages", "Using MPI-2: A problem-based approach", "Toward understanding soft faults in high performance cluster networks", "Installation and user's guide to MPICH, a portable implementation of MPI version 1.2. 5", "Beowulf cluster computing with Linux", "Installation and User\u2019s Guide to MPICH, a Portable Implementation of MPI Version 1.2. 5", "PETSc Web page. h ttp", "PETSc and Overture: Lessons learned developing an interface between components", "Analyzing the parallel scalability of an implicit unstructured mesh CFD code", "Using MPI", "MPI: The Complete Reference. Scientific and Engineering Computation Series", "MPI: The complete reference", "The second-generation adi for the mpich implementation of mpi", "Experiences with the IBM SP1", "Semi-structured refinement and parallel domain decomposition methods", "Domain decomposition with local mesh refinement", "An introduction to performance debugging for parallel computers(mcs-p500-0295)", "PETSc: Portable, extensible toolkit for scientific computation,(2001)", "Decoupled I/O for data-intensive high performance computing", "Using Advanced MPI: Modern Features of the Message-Passing Interface. Scientific and Engineering Computation", "Weighted locality-sensitive scheduling for mitigating noise on multi-core clusters", "OpenMP in the Petascale Era: 7th International Workshop on OpenMP, IWOMP 2011, Chicago, Il, USA, June 13-15, 2011, Proceedings", "Program optimization through loop vectorization", "MPI and hybrid programming models for petascale computing", "Parallel programming models applicable to cluster computing and beyond", "Fault tolerance in message passing interface programs", "Mpich abstract device interface version 3.4 reference manual draft", "Fault tolerance in MPI programs, Special Issue of the J", "PETSc home page. h ttp", "MPI: The Complete Reference, Volume 2-The MPI-2 Extensions, vol. 2", "The Portable, Extensible, Toolkit for Scientific Computing (PETSc) ver. 22", "Wi11iam Saphir, and Marc Snir. MPI The Complete Reference: Volume 2, The MPI-2 Extensions", "PETSc 2.0 Users Manual: Revision 2.0. 16", "PETSc World Wide Web home page", "Lusk, E.\u2014Doss, N.\u2014Skjellum", "MPICH ADI Implementation Reference Manual-DRAFT", "Using the scalable nonlinear equations solvers package", "PETSc 2.0 User's Manual", "The implementation of the second generation MPICH ADI", "Krylov methods with incomplete factorization preconditioners on the cm-2", "Domain decomposition for nonsymmetric systems of equations-Examples from computational fluid dynamics", "Dynamic Grid Manipulation for PDEs on Hypercube Parallel Processors", "Numerical analysis program library user\u2019s guide (NAPLUG)", "E. Lusk. 1996", "A., S. 1999. Using MPI: Portable Parallel Programming with the Message-Passing Interface", "A High-Performance, Portable Implementation of the MPI Message Passing Interface Standard, Argonne National Laboratory, 1996", "Using MPI, 1999", "Users Guide for ROMIO: A High-Performance, Portable MPI-IO Implementation. Mathematics and Computer Science Division, Argonne National Laboratory, October 1997", "Domain Decomposition, Parallel Multilevel Methods for Elliptic Partial Differential Equations,(1996)", "Using MPI-2: Advanced Features of the Message Passing Interface, 1999", "FFT, FMM, and multigrid on the road to exascale: Performance challenges and opportunities", "Using node and socket information to implement MPI Cartesian topologies", "Using performance models to understand scalable Krylov solver performance at scale for structured grid problems", "Eliminating contention bottlenecks in multithreaded MPI", "Scalability challenges in current MPI one-sided implementations", "Collective algorithms for multiported torus networks", "Lightweight Scheduling for Balancing the Tradeoff Between Load Balance and Locality.(2014)", "CFD Vision 2030: A path to revolutionary computational aerosciences", "Toward Exascale Resilience: 2014 update. Supercomputing Frontiers and Innovations 1, 1 (2014)", "Teaching parallel programming: a roundtable discussion", "Multi-Core for HPC: breakthrough or breakdown?", "FPMPI-2 fast profiling library for MPI", "Implementing MPI-IO atomic mode without file system support", "Parallel programming with MPI", "Parallel programming considerations", "The CH3 Design for a Simple Implementation of ADI-3 for MPICH with a TCP-based Implementation", "Parallel I/O", "MPICH Abstract Device Interface, Version 3.3", "Infrastructure and interfaces for large-scale numerical software.", "Using MPI-2: Advanced Features of the Message", "The MPI-2 Extensions, volume 2 of MPI: The Complete Reference", "MPI-The Complete Reference, Volumes 1 and 2", "An introduction to MPI: Parallel programming with the message passing interface", "MPI--the Complete Reference. Vol. 2, The MPI-2 Extensions (Scientific and Engineering Computation Series)", "Why we couldn\u2019t use numerical libraries for PETSc", "MPICH: A highperformance, portable implementation for the MPI Message-Passing interface", "Parallel domain decomposition software", "Solutions of team problems 13 and 20 using a volume integral formulation", "Using MPI portable parallel programming model with Message Passing Interface", "Using MPI MIT Press", "Solutions of TEAM Problem No. 13 using integral equations in a sequential and parallel computing environment", "Blockcomm for fortran", "Computational Fluid Dynamics on Parallel Processors.", "An abstract device interface for implementing portable paralllel-I", "FPMPI: A fine-tuning performance profiling library for MPI, November 2001", "Users' Guide to mpich, a Portable Implementation of MPI\", 1995", "PETSc Users Manual Technical Report ANL-95/11-Revision 2.1. 3, Argonne National Laboratory, 2002", "Domain Decomposition. 1996", "Learning with analytical models", "Performance modeling of distributed deep neural networks", "MPI+ MPI: Using MPI-3 Shared Memory As a Multicore Programming System", "Efficient disk-to-disk sorting: a case study in the decoupled execution paradigm", "Rethinking key-value store for parallel i/o optimization", "PETSc Users Manual", "Stable numerical methods for hyperbolic partial differential equations using overlapping domain decomposition", "PETSc developers manual", "Architectural constraints required to attain 1 Exaflop/s for scientific applications", "An adaptive performance modeling tool for GPU architectures Proceedings of the 15th ACM SIGPLAN symposium on Principles and practice of parallel programming (PPoPP\u201910)", "Implementation and Shared-Memory Evaluation of MPICH2 over the Nemesis Communication Subsystem. sl: Mathematics and Computer Science Division", "Grid-based image registration", "Design and implementation of MPICH2 over InfiniBand with RDMA support. CoRR", "Building library components that can use any mpi implementation", "An evaluation of a user-level data transfer mechanism for high-performance networks.", "Improving the performance of sparse matrix-vector multiplication by blocking", "Achieving high performance with MPI-IO", "Using Mpi-2: Advanced Features of the Message Passing Interface. 2nd", "Introduction to Performance Issues in Using MPI for Communications and I/O", "MPI: The complete reference, volume 2, Scientific and Engineering Computation", "MPICH home page", "Porting the MPICH MPI implementation to the sun4 system", "DD, Parallel multilevel methods for elliptic PDEs", "Installation Guide to mpich, a Portable Implementation of MPI Version 1.2. 2", "Parallel programming tools user's manual", "Domain decomposition with local mesh refinement", "Parallel performance of domain-decomposed preconditioned Krylov methods for PDEs with adaptive refinement", "A domain decomposition method with locally uniform mesh refinement", "Local uniform mesh refinement on loosely-coupled parallel processors", "Toward exascale resilience: 2014 update, Supercomputing frontiers and innovations 1 (1), open Access", "PETSc, portable extensible toolkit for scientific computing (web site)", "User's Guide for MPE: Extensions for MPI Programs. Argonne National Laboratory, 1998", "PETSc Web page [online](2015)", "Reproducible measurements of MPI performance characteristics, 1999", "Annotations for performance and productivity. 2007", "On Implementing MP1-IO Ponably and with High Perforrnance", "Data sieving and collective I", "Toward faster packing and unpacking of mpi datatypes", "A user-defined schedule for OpenMP", "Is MPI+ X enough for exascale", "Analyzing the performance of a sparse matrix vector multiply for extreme scale computers", "MPICH Installer\u2019s Guide", "CFD vision 2030 study: a path to revolutionary computational aerosciences", "Lightweight scheduling for balancing the tradeoff between load balance and locality", "Runtime system design of decoupled execution paradigm for data-intensive high-end computing", "Portable, Extensible Toolkit for Scientific Computation", "Parallel adaptive deflated GMRES", "Slack-Conscious Lightweight Loop Scheduling for Improving Scalability of Bulk-synchronous MPI Applications", "Scalable memory use in MPI", "Generalizing smoothed aggregation-based algebraic multigrid", "HadoopJitter: The Ghost in the Cloud and How to Tame It", "On the need for a consortium of capability centers", "Advanced MPI: I/O and one-sided communication", "Towards a productive MPI environment", "Cray x1 evaluation", "Parallel netcdf: a high-performance scientific i/o interface", "Exploring the relationship between parallel application run-time variability and network performance in clusters. Local Computer Networks", "Parallel computer architectures", "Interfacing parallel jobs to process managers", "Advanced topics in MPI programming", "Installation and User\u2019s Guide to MPICH, a Portable Implementation of MPI Version 1.2. 6 The ch nt device for workstations and clusters of Microsoft Windows machines", "Understanding the parallel scalability of an implicit unstructured mesh cfd code", "Coding Standards and Development Framework", "Runtime Checking of Datatype Signatures in MPI in Jack Dongarra, Peter Kacsuk, Norbert Podhorszki (Eds.)\u2019Recent Advances in Parallel Virtual Machine and Message Passing Interface\u2019", "Parallel computation of three\u2010dimensional nonlinear magnetostatic problems", "On implementing MPI-IO portably and with high performance", "Data sieving and collective I/O in ROMIO. Frontiers of Massively Parallel Processing", "Using MPI: portable parallel with message-passing interface", "The MPICH Implementation of MPI", "A case for using MPI's derived datatypes to improve I", "MPI-The Complete Reference: Volume 2, The MPI-2 Extensions, volume 2, The MPI-2 Extensions", "MPICH Model MPI Implementation", "An introduction to performance debugging for parallel computers", "The implementation of the second generation MPICH ADI. MPICH Working Note", "PETSc 2, 0 Users'", "Portable Implementation of the MPI", "Users manual for tohtml: Producing true hypertext documents from LaTeX", "Parallel implicit solvers for steady, compressible aerodynamics", "Portable, parallel, reusable Krylov space codes", "Scalable Unix tools on parallel processors. In 1994 Scalable High-Performance Computing Conference", "Anthony skjellum,\"", "BlockComm for Fortran (unpublished),\"", "The PETSc package", "Domain decomposition as a mechanism for using asymptotic methods", "Domain decomposition", "Parallel Processing for Scientific Computing, chapter A Parallel Version of the Fast Multipole Method", "Computational fluid dynamics on parallel processors, Air Force Office of Scientific Research", "Local Uniform Mesh Re\ufb01nement with Moving Grids \u2018", "Installation Guide for mpich, a Portable Implementation of MPI. Mathematics and Computer Science Division, Argonne National Laboratory, 1996", "Tuning MPI programs for peak performance (1997). Argonne National Laboratory", "Using mpi-2: A problem-based approach, 2007", "Noncontiguous access through MPI-IO", "Installation and User\u2019s Guide to MPICH, a Portable Implementation of MPI, The ch_p4 device for Workstation Networks", "Fault tolerance in MPI programs mathematics and computer science", "User\u2019s Guide for MPE: Extensions for MPI Programs, 1998", "Portable Parallel Programming with the Message-Passing Interface, 1994", "PETSc 2.0", "Reducing communication in algebraic multigrid with multi-step node aware communication", "Convergence of Artificial Intelligence and High Performance Computing on NSF-supported Cyberinfrastructure", "Deep Learning for Multi-Messenger Astrophysics: A Gateway for Discovery in the Big Data Era", "Rethinking High performance computing system architecture for scientific big data applications", "Composing low-overhead scheduling strategies for improving performance of scientific applications", "Using Advanced MPI.", "Parallel libraries", "Dynamic Process Management", "An Introduction to MPI", "SC13\u2013The International Conference for High Performance Computing, Networking, Storage and Analysis", "Optimization Strategies for MPI-Interoperable Active Messages", "CFD vision 2030 study: a path to revolutionary computational aerosciences", "Rethinking high performance computing system architecture for big data applications", "Performance modeling as the key to extreme scale computing.", "Performance evaluation and enhancement of Dendro", "Hypergraph-Based Combinatorial Optimization of Matrix-Vector Multiplication", "Self-consistent MPI performance requirements", "Scaling Science Applications on Blue Gene.", "Parallel Implicit Solution of Diffusion-limited Radiation Transport", "Data Transfer in a SMP System: Study and Application to MPI", "MPI and high productivity programming", "The 2-D Poisson Problem", "MPI on the Grid", "A tour of Jumpshot-3", "Parallel I/O", "Using MPI: Portable Parallel Programming With the Message-passing Interface (Scientific and Engineering Computation)", "Newton-Krylov-Schwarz algorithms for the 2D full potential equation", "Installation Guide to mpich, a Portable Implementation of MPI", "Early experiences with the IBM SP-1", "CLAM: A programming language incorporating expert use of linear solution algorithms", "CLAM and CLAMShell: A system for building user interfaces", "Balanced Divide-and-Conquer Algorithms for the Fine-Grained Parallel Direct Solution of Dense and Banded Triangular Linear Systems and their Connection Machine Implementation", "Domain Decomposition Techniques for Large Sparse Nonsymmetric Systems Arising from Elliptic Problems with First-Order Terms", "Recursive mesh refinement on hypercubes", "Numerical Solution of Transport Equations.", "Installation Guide to mpich", "User's Guide for", "Installation and User\u2019s Guide to MPICH, a Portable Implementation of MPI Version 1.2. 5 The globus2 device for Grids", "Translational Research in the MPICH Project", "HAL: Computer System for Scalable Deep Learning", "Node-Aware Improvements to Allreduce", "Managing code transformations for better performance portability", "Guest editor's introduction: Special issue on best papers from EuroMPI/USA 2017", "DAME: Runtime-compilation for data movement", "Strategies for Dynamic Load Balancing on Highly Parallel Computers", "Moya\u2014A JIT Compiler for HPC", "A DSL for Performance Orchestration", "Final report for \u201cExtreme-scale Algorithms and Solver Resilience\u201d", "ICS 2017 general chairs' welcome", "Why we couldn\u2019t use numerical", "Compiled MPI. Cost-Effective Exascale Application Development", "Runtime support for irregular computation in MPI-based applications", "Architecture-Aware Algorithms for Scalable Performance and Resilience on Heterogeneous Architectures. Final Report", "Applying Performance Modeling to Guide Hybrid MPI/OpenMP Use and Communication/Computation Tradeoff in Algebraic Multigrid", "Scientific Programming Volume 22 Issue 2", "CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences", "Fun with Datatypes", "Hybrid Programming", "Coping with Large Data", "Using Shared Memory with MPI", "Support for Performance and Correctness Debugging", "Advanced Remote Memory Access", "Working with Modern Fortran", "Working with Large-Scale Systems", "Glossary of Selected Terms", "Features for Libraries", "Introduction to Remote Memory Operations", "Function and Term Index", "Understanding How MPI Implementations Work", "A MPI Resources on the World Wide Web", "Comparing MPI with Sockets", "Other Features of MPI", "Language Details", "The MPI Message Queue Dumping Interface", "Final Report for Enhancing the MPI Programming Model for PetaScale Systems", "PRAC: Systems Software for Scalable Applications", "A Considered Approach to Multiphysics Problems at the Exascale: Coupled Until Proven Decoupled", "Performance modeling framework for SLO-driven MapReduce environments", "DOMAIN DECOMPOSITION TECHNIQUES FOR LARGE SPARSE NONSYMMETRIC", "ADAPTIVE METHODS FOR HYPERBOLIC PROBLEMS", "Building an Open Community Runtime (OCR) framework for Exascale Systems", "Advanced MPI including new MPI-3 features", "Conservation and efficiency in least squares finite element methods", "Emergence and stability of complex structures from stochastic neuronal networks", "Leveraging MPI's One-Sided Communication Interface for Shared-Memory Programming.", "Best algorithms+ best computers= powerful match", "Task Mapping in Hierarchical Direct Networks", "PRACE Summer School 2011-Advanced MPI: Part 4", "ICS", "Performance Modeling for Systematic Performance Tuning", "Proceedings of the International Conference on Parallel Processing Workshops: Welcome Message", "Grid-based Image Registration", "Keynote", "Enabling the Next Generation of Scalable Clusters.", "Erratum: The importance of non-data-communication overheads in MPI (International Journal of High Performance Computing Applications (2010) 24: 1", "The Importance of Non-Data-Communication Overheads in MPI (vol 24, pg 1, 2010)", "EuroPVM/MPI Full-Day Tutorial. Using MPI-2: A Problem-Based Approach", "Improving the performance of tensor matrix vector multiplication in quantum chemistry codes.", "John Mellor-Crummey, Rice University", "MPI MULTIPLE THREAD", "Electron injection by a nanowire in the bubble regime", "MPI web page", "Observations on WoCo9", "Application supercomputing and multiscale simulation techniques", "Where does MPI need to grow?", "Parallel Tools and Environments: A Survey", "Design and implementation of message-passing services for the Blue Gene/L supercomputer", "International Journal of High Performance", "Minimizing Synchronization Overhead in the Implementation of MPI", "International Journal of High Performance", "High-Level Programming in MPI", "Future developments in MPI", "\u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043f\u043e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 PETSc", "Introduction to PETSc", "Reproducible Measurements of MPI", "SOFT FAULTS IN HIGH PERFORMANCE CLUSTER NETWORKS", "Cluster 2001: special issue of Concurrency and computation: practice and experience", "Cluster 2001", "Extensions, Improvements and Implementations of PVM/MPI-MPI on BlueGene/L: Designing an Efficient General Purpose Messaging Solution for a Large Cellular System", "PETSc \u7528\u6237\u624b\u518c", "Advanced Cluster Programming with MP.", "Challenges and Successes in Achieving the Potential of MPI", "Achieving high sustained performance in an unstructured mesh CFD application", "MPICH, Message Passing Interface (MPI) Subroutine Library for Parallel Computers and Networks", "Intermediate MPI", "Using MPI in Simple Programs", "Comparing MPI with Other Systems for Interprocess Communication", "The MPE Multiprocessing Environment", "Summary of MPI-1 Routines and Their Arguments", "Advanced Message Passing in MPI", "Using MPI-2: Advanced Features of the Message-passing Interface (Scientific and Engineering Computation)", "Prof. Makoto Amamiya, Kyushu University Dr. Utpal Banerjee, Intel Corporation Prof. Andrew Chien, UC San Diego Prof. Kyle Gallivan, Florida State University Prof. Dennis Gannon\u00a0\u2026", "Domain Decomposition: Parallel Multilevel Methods for Elliptic Partial Differential Equations", "Which comes first: The architecture or the algorithm?", "Parallel Newton-Krylov-Schwarz Algorithms for the Transonic Full Potential Equation", "Parallel Newton-Krylov-Schwarz Algorithms for the Transonic Full Potential Equation.", "PETSC 2.0. Portable Extensible Toolkit for Scientific Computation", "A Taxonomy of Programming", "Computational Electromagnetics and Parallel Dense Matrix Computations.", "ANL IBM SP1", "Anthony Skjellum", "PIO Reference Manual", "NASA-LaRC, Hampton, VA 23681 USA", "Overlapping Schwarz Methods", "Parallel solution of the three-dimensional, time-dependent Ginzburg-Landau equation", "Domain Decomposition Methods for Transonic Flow", "PARALLEL PROGRAMMING TOOLS FOR DISTRIBUTED MEMORY COMPUTERS", "MPICH. Portable Implementation of the Standard Message Passing Interface", "Part 2: Asymptotic-induced Domain Decomposition Methods", "Parallel implicit domain-decomposed solvers for PDEs", "Domain-decomposed preconditionings for transport operators", "Domain decomposition methods in computational fluid dynamics(Final Report)", "Domain decomposition with local mesh refinement(Final Report)", "Domain Decomposition Method", "Parallel performance of domain-decomposed preconditioned Krylov methods for PDES with adaptive refinement. Research report", "Parallel Domain Decomposition with Local Mesh Refinement", "Domain decomposition with local mesh refinement. Research report", "Krylov Methods Preconditioned with Incompletely Factored Matrices on the CM-2", "CFD (computational fluid dynamics) research for mini-supercomputers: a Yale/UTRC program. Final report, 15 February 1987-14 October 1988", "CFD (Computational Fluid Dynamics) Research for Mini-Supercomputers: A Yale/UTRC Program", "Krylov Methods Preconditioned with Incompletely Factored Matrices on the CM-2", "Krylov methods preconditioned with incompletely factored matrices on the CM-2(Final Report)", "Research Center for Scientific Computation Yale University", "Research Report YALEU/DCS/RR-616", "Research Report YALEU/DOS/RR-616 March 1988", "Adaptive Methods for Hyperbolic Problems on Local Memory Parallel Processors", "A Gray-Code Schmee for Mesh Refinement on Hypercubes", "Local Uniform Mesh Refinement for Partial Differential Equations.", "Local uniform mesh refinement for partial differential equations(Final Report)", "Research Report YALEU/DCS/RR-458", "Research Report YALEU/DCS/RR-458 March 1986", "Dynamic grid manipulation for PDES (partial differential equations) on hypercube parallel processors. Research report", "Dynamic Grid Manipulation for PDEs (Partial Differential Equations) on Hypercube Parallel Processors.", "on Loosely-Coupled Parallel Processors", "Research Report YALEU/DCS/RR-352", "High performance bulk memory system. Research report", "A High Performance Bulk Memory System.", "Elliptic Partial Differential Equations", "2 Research, Scholarly, and Creative Activities", "Numerical solution of transport equations[Ph. D. Thesis]", "Shades of Grey", "Center for Scalable Application Development Software: Management Plan", "Reducing Communication Costs in the Parallel Algebraic Multigrid", "Jack Dongarra Pete Beckman Terry Moore Patrick Aerts", "Appeared in Proc. of the \u0417rd Int6l Conf. of the Austrian Center for Parallel Computation with special emphasis on Parallel Databases and Parallel IeO, Sept. 1rrs. Lecture Notes\u00a0\u2026", "Efficient Implementation of MPI-2 Passive", "A User-defined Loop Schedule in OpenMP", "Processors: A High-Performance Implementation", "MEC\u00c1NICA DE FLUIDOS COMPUTACIONAL", "MPI THREAD MULTIPLE", "Appeared in Proc. of the 6th Symposium on the Frontiers of Massively Parallel Computation, October 1996, pp. 180-187. c 1996 IEEE. An Abstract-Device Interface for Implementing\u00a0\u2026", "Scalable Ecosystems for Data Science (SEDS)", "Operational Issues and Deployment Issues for Clusters and Grids", "Solving PDEs with PETSc", "Domain Decomposition with PETSc", "User's Guide for mRSUV, a Portable Implementation of MPI Version 1.2.", "1st Annual PDSW-DISCS Workshop", "Section A: Performance Benchmarking and Optimization", "E. Clementi, D. Logan, V. Sonnad A Look at the Evolution of Mathematical Software for Dense Matrix Problems over the Past Fifteen Years....................... 29 JJ Dongarra\u00a0\u2026", "Algorithms and Architecture", "MPI-IO: A Standard, Portable API for High-Performance Parallel I/O", "IN DIE VERBYGAAN: PASSING EVENTS", "The single most important impediment to good parallel performance is still poor singlenode performance.", "The 2014 International Workshop on Data-Intensive Scalable Computing Systems", "PARCO 1590", "Henri Casanova, The University of Hawaii, USA Pete Beckman, Argonne National Lab, USA Al Geist, Oak Ridge National Lab, USA Rajkumar Buyya, Melbourne University, Australia Greg\u00a0\u2026", "Organizing and Program Committee", "Technical Program Committees", "Towards Generalized, Asynchronous, and MPI-Interoperable Active Messages", "\u00c5\u00c8\u00c1 \u00bd\u00ba\u00be\u00ba\u00be", "Search eLibrary", "Clusters", "Vortr\u00e4ge und Posterpr\u00e4sentationen (ohne Tagungsband-Eintrag)", "Scalable communicators for exa-scale computing", "with special emphasis on Parallel Databases and Parallel IeO, Sept. 1qqr An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a\u00a0\u2026", "Formal Methods Applied to HPC Software Design: A Case Study of Locking Based on MPI One-Sided Communication", "MPICH2 Design Document Draft of October 8, 2003", "P2S2 2010", "User's Guide for \u03a1\u03a4, a Portable Implementation of MPI Version l. 2.0", "CCGrid 2010", "W hy Are PVM and M PI So Different?", "MPICH Abstract Device Interface Version 3.4 Reference Manual Draft of September 3, 2003", "P2S2-09 Committees", "Efficient Communication Across the Internet in Wide-Area MPI", "Installation and User\u2019s Guide to MPICH, a Portable Implementation of MPI Version 1.2. 6 The ch p4mpd device for Workstation Networks and Clusters", "Von Megahertz zu Gigaflops", "Software for the Scalable Solution of PDEs", "JACK DONGARRA", "The MPIR Process Acquisition Interface", "Installation and User's Guide to \u00c5\u00c8\u00c1 \u00c0, a Portable Implementation of MPI Version I. 2.3 The \u00d0\u00d3 \u00d9\u00d7 \u00be device for Grids", "Message from the Technical Program Chair", "Coding Standards and Development Framework Draft of November 4, 2003", "User's Guide for \u00c5\u00c8: Extensions for MPI Programs", "Introduction to PETSc", "Data parallel loop statement extension to CUDA: GpuC", "EuroPVM/MPI'03", "A Tour of umpshot", "Installation and User's Guide to \u00c5\u00c8\u00c1 \u00c0, a Portable Implementation of MPI Version I. 2.3 The \u00d4 device for Workstation Networks", "Installation Guide to mpich, Version 1.2. 2", "Installation and User's Guide to \u00c5\u00c8\u00c1 \u00c0, a Portable Implementation of MPI Version I. 2.3 The\u00d7 \u00d1 \u00d1 device for Shared Memory Processors", "a Portable Implementation of MPI Version 1.2. 2", "HSLN 2004 Workshop Committee", "Demonstration of a Prototype System-Aware Data Transfer Mechanism for Computational Grids", "Exascale Computing", "Installation and User\u2019s Guide to MPICH, a Portable Implementation of MPI Version 1.2. 6 The ch shmem device for Shared Memory Processors", "High Performance Wide Area Data Transfers Over High Performance Networks", "N IC-B ased A tom ic R em ote M em ory O perations in", "Fourth International Workshop on Parallel Programming Models and Systems Software for High-End Computing", "Installation and User's Guide to \u00c5\u00c8\u00c1 \u00c0, a Portable Implementation of MPI Version I. 2.3 The \u00d4 \u00d1\u00d4 device for Workstation Networks and Clusters", "Reviewing Committee", "Solutions of TEAM Problems 13 and 20 Using a Volume Integral Formulation", "Exploiting Hierarchy in Parallel Computer Networks to Optimize Collective Operation Performance Nicholas T. Karonis High-Performance Computing Laboratory Department of Computer\u00a0\u2026", "ANL IBM SP-1 DRAFT", "MPICH Abstract Device Interface Version 3.3 Reference Manual Draft of December 10, 2001", "Appeared in Proc. of the 6th Symposium on the Frontiers of Massively Parallel Computation, October 1996, pp. 180-187. c 1996 IEEE."], "ids": ["3d268dda-b740-4bb6-88fd-768879dd7f61", "e4e92e07-a222-4386-9f7b-cdcf1ddaf2f3", "7be87213-8359-470c-a6ac-50ad51b19b1f", "b11313ba-e82e-4016-85c1-a7ec088c9c90", "21280217-0a1b-4faa-88d8-227c3e06ea18", "78390715-6447-40ee-9982-6c7953b89d44", "2e374f1d-18a2-41b5-a821-11c9d4c3b35e", "75a120fc-0235-461b-a535-e8dde6ea8655", "1db153a3-46f2-4055-8ab7-3e7a000fdc0d", "960deb82-00db-4dbd-b567-85d5e850196b", "ab917d5b-b317-44f3-9700-026c2f0549ec", "8dc3d096-4311-4eb4-8caf-6c4834633ec0", "10ebd964-5c95-4c8e-af18-8d7975483fb7", "3b0b30de-8b62-4fce-993d-15da3d25bff9", "cf27735b-75b7-46fa-9fdc-8a001d24e77c", "0ebd5680-c972-47df-8645-d3b709633814", "af304e60-b091-4d5d-a8ac-10d7126debfb", "05e8a5f4-fe65-4584-925d-d61303fada36", "bb83c27a-3cca-4ecb-a363-808175fe3723", "9b59ad4b-9a1f-492d-9ffb-929156a5a6c8", "8dcca927-4778-44ef-96b2-47304a4c4bec", "db6547b0-888d-4365-95e3-fd16dd67f29b", "75065b79-1ca0-42b1-83d1-35b5caae1702", "e6f146ee-8e16-42a2-814d-2229cb121fd3", "3683ce5b-e72b-4965-bcce-3ecea3a85d5d", "fb70fd72-f7d7-4323-9533-d8a3c23094f6", "def03a4b-3730-4c43-a667-98e834de8a2e", "4530ac8a-90cf-4159-be29-6c256f879d24", "7289a91c-7fdd-4a86-8c84-cc84dae21ec7", "1cd7284b-fc1d-4ee1-859f-e5b70d03e0aa", "b472a441-1c7f-48a9-90df-f03306181097", "5e27c89c-4e9c-42c7-bf12-c12eb6b6a9a3", "5ae2047c-9533-4628-b29c-d2618db64590", "fe2b2fdb-7e40-4c62-914e-53e84a5fc846", "9cc44086-fc54-4e00-8491-46779ab8b38c", "417ec3a0-d215-4aa0-88bf-006c970d0217", "a077631d-c25b-49af-80d9-0bedb75de184", "e0215d41-dc43-45bd-917b-6527ec156753", "71cefc52-9c94-42ff-887a-9462737b4d92", "7d51b151-85ff-4e61-94f5-aeb468500f1c", "f8a6a013-eb54-4f18-ae21-66b64f88437f", "0b62aaf3-7edf-4069-8069-77c742eb1cc0", "b4792958-e314-4180-a42b-09115f67daac", "87ba6de5-ae7f-431c-a68f-15c51be98ea2", "4a886d74-7f2e-4657-9d2e-b3290a75e511", "1790e5de-1f62-4b2c-aae3-6d8848938da7", "9365dda9-d9a6-4e23-b3d5-85226081e9be", "e8dfeae4-3d61-43a0-b4df-0c8cfb817ebd", "145003de-1d4d-4838-a50c-c2e107483b15", "e0c4bad1-5dde-42dc-8b91-759cc8486c96", "5caaa55a-9d2b-41d1-9f98-93d544e1fe5e", "1aa66a12-36f3-4e95-874f-9416a5da2b71", "6be96561-7d2f-4bbd-b745-9f5643664175", "25654812-cf15-4bb1-b417-882957a43c75", "370085de-22a9-41d2-a500-5f43fcb184f3", "716093fe-8de3-4eea-a742-e367aa065501", "2fc78144-1473-405f-9742-b081463d7ffd", "eb6160d8-9b9a-4d1d-b000-02667c45c550", "8e567b6d-5b0e-4e1d-b7f3-4acc7ca46e18", "1504e6e3-1b54-430c-8389-2446dbee951c", "6aa5bea1-6e94-4bd5-950f-6d6162545bc8", "2ca8f050-56b8-4d24-a33c-2414ab92052a", "c2e30f8f-771f-4950-8db9-2d4e63be77b7", "b74b4388-3c4f-41f7-92ea-a8913c4c7d8b", "ae9ac4c4-b5f9-47ce-a76a-4102f835e486", "5a0582d2-4ace-4a0f-be58-502ca5a50728", "594e4079-ad88-488b-8fe6-8ff06321ff7d", "3dd5eea2-1139-408a-be9c-89293769ec72", "8d1e4c51-3a7e-4d23-8a3d-849a12c342dd", "3b11efbd-3882-41b6-8ed1-740e2d2e5ec2", "39b3a741-26d2-451f-9b7c-d5bb8e0beecd", "6d0d738e-f73e-4ca3-b692-1bced033c960", "fed6fff5-9e3b-4be7-abb2-27b574b4713e", "28b377f1-6be6-4589-bb0d-802e66386add", "77388382-ffd2-4ebb-915e-91e57d6fcdbf", "fc76809e-905b-4f7c-a8f4-370c79a30a60", "ca621c69-d8b3-4948-810d-475c660df1a0", "3d2a262e-fb03-4774-bb57-fbf500ac43ca", "885f7b14-9d72-46a7-bb4d-e958b90626d0", "67b8aec0-f0af-43d4-8d87-d71849cc1936", "395ac3f5-c2b5-4fb8-b21f-318ddb2feb45", "c6a2b35f-94dc-43c8-b6b5-cd289ea900c5", "bb68dee0-129e-4510-83ad-0adc4bf105d5", "9fd04b35-78ed-4a66-a2e9-271d836ba10e", "b9e7b7d3-6b76-406a-a88c-7103f04569f7", "47e287e4-b47d-4c5f-a666-63f4327dd8c6", "2805ab22-ec15-41db-8537-55654bf4b2f4", "c7c6b368-0627-49cb-b276-aab16d3d4c3f", "ef283c93-b47e-4333-967d-2fd3a263ac01", "43daa28c-9e5c-4bef-8ab5-9ea7b1b1455d", "0b7cc8bf-3bbf-40f2-af0d-9790335dd880", "6db79ac2-65dc-4e4b-9a21-ae64b8bbc842", "8ff57a43-81ca-419b-80e0-cfe39f438395", "9c5ee913-9856-4ff3-9b0f-5588be30f9b9", "8c60474f-e65e-437f-ac4e-74205f84b307", "e966b24d-68fa-40d3-b5fb-cef5a11f46eb", "15655c57-0103-46bc-aff4-f360455bcacd", "bc89b113-978c-44a1-a426-c6646cb8ff12", "adbc4ad1-133c-42aa-8810-dc22093bc572", "d76db1e6-2c50-46f6-86c2-18fc9336188c", "f49bfa8a-0a3d-46d5-a579-9f0064951e72", "bfa4b59d-a101-459a-b370-8e3461a93680", "c6773755-9214-4d7e-bdbb-4f6c108b179c", "02846de9-fc21-4c6c-b9e0-add9100573ab", "5cef0b04-970c-4446-baf7-048b1b505ff0", "36c9ed9a-b5f4-4352-8638-adf2f8e01ded", "db055a42-01de-435a-8d8a-e24d908617ba", "ec60768f-418a-4f71-8ed5-a238b7ec48d3", "28c6cbd8-8b9c-458c-83e9-f85c09e87673", "ea965983-d2c0-425f-9924-30d3e8c32852", "54e939bb-f472-465b-9471-8f67c6b59edb", "93635523-fd20-4f0e-84da-897e51d9eca8", "41a3546d-a22b-496c-a3c4-36b4b90f17b1", "3c68231d-5705-4dfe-a5d7-55083261768d", "0ad6db16-586b-4598-9f5c-a00ba5edf4cd", "90e89d28-3bf1-4f89-8a16-1948a06b09b1", "00a9ad9e-1902-4b4b-84f2-4da3fd7bd92c", "6b520667-00ab-4cee-a18a-aa272b74e50b", "c206b533-669d-4ec6-a9cc-37e25d2a6765", "cb7e018d-59ae-4f97-997b-4d9d89c9eec7", "2b4b2b60-f545-4bbe-bd8a-e0994cfb54af", "fb648077-cf53-4b6f-bde8-0a93de0db53f", "9887124e-807b-40c6-80e6-e7c53dd1351f", "e72a102b-6251-4ea1-af30-bdb4beacdeee", "60ee7e04-79c5-4af4-98b3-5bfafcc5cc4b", "60841236-b5ab-4e95-ad71-2e0675ab04fb", "60c07d23-bb3e-4759-bf13-ae1def57e4e7", "8ed4f57e-0b8a-474c-a834-84da75c9fa3e", "f03162a0-6f9d-41e1-8485-174340a69b2e", "205527b7-24d0-475d-900d-e081ded73b56", "ab917d5b-b317-44f3-9700-026c2f0549ec", "c370f21b-f824-4133-bc5c-a7e8cc02f30c", "520c1d9c-ecfc-491e-977c-03d466f3ba05", "485e51f8-d7a0-41e3-8bfa-84d3c6dd15f9", "60c73c2a-081b-4127-88c2-cef4b4192189", "d3fcdcce-50a7-4688-b253-a8be19463d04", "031a27a6-39d0-4f6e-b9fa-dc9327e29130", "554304cf-0841-4821-8932-f126a2118172", "447aafa2-3d99-42bf-bc50-0caab4b843a7", "cd1c2c54-fb40-4a01-ae4f-aa2de2fe0533", "cfafca68-edda-4b9c-a7b5-a092f11570c4", "fc0ffc44-eb02-48b6-a749-8c2be19cf57f", "f335d1cf-a111-47be-8e5e-fd4c3d1d4f55", "acefa770-e3c4-4564-8885-ed3f488b4f97", "d686772c-3a8d-4a99-bff2-0baa571377dc", "ce934a78-6266-4bb0-89a1-15e9c7e3e01a", "748ca775-24ad-438b-9410-66ec4513ea14", "20053ea2-6ff1-4d87-bdd7-465b945ba9d9", "2f401feb-8308-4afd-b310-2aaf90c02b23", "0ea30919-6662-4108-8505-ee1e82a766d5", "5c220d33-366b-4d74-aba7-c7f83679eb44", "28a3d4f5-15de-49ae-8680-e7772fc192da", "d6fa973d-3447-456e-ae08-ced139c343b8", "4a2c31d8-8704-4e95-a8b8-ce2bb7a2aeab", "57960af5-a6ea-4ba5-a719-30b9ae4ff4ab", "63f08df9-77b2-490e-b9cf-1cb9a2bb047b", "3f475124-489b-483d-96ac-abd54d6604b0", "ef1b8036-c2a4-4ed5-90e4-a1168a770a03", "97f3781a-3fb1-406d-b400-8354c5d8d705", "888c3bfe-2b81-47ff-9e6c-55ae51aa6b63", "3d268dda-b740-4bb6-88fd-768879dd7f61", "71068d09-a3b3-4a23-b130-d7cb9603263b", "3cd9f31f-9db4-4104-84b5-e877ef236683", "df8b0648-051f-41c2-9963-e1e9f1729417", "6c0cf199-eb3b-43ee-bf56-a6bf3d8891be", "e441179a-c4f8-43db-b9a3-16269cf2d0fc", "feafb3ef-00a8-4e96-89a5-724d0c33eeeb", "0cc834c0-6b38-4b7e-a2d0-95ad7a9ae62f", "750811c0-186d-4fc8-94ee-f45be11084dc", "934359d7-877a-4c95-a584-ba055a557ea0", "decfd7cd-775a-4bed-986a-27bb170cfc43", "de020e58-6192-4d08-8d94-736a05950eef", "9ce820df-bfff-4bb8-98ce-2b76a15aede0", "2164ccdb-f252-421c-b3fd-314d5cc1620d", "a00df993-7b4b-4415-aec1-883c90dc51a3", "86360e1c-8f19-443e-8c21-1a1284a8f45b", "757ee2f7-7482-4f71-b20a-54472dd2328c", "c96d0663-773a-4533-885a-76c067b1ace1", "be741c86-7fdc-48ef-9be7-a666bf24eea1", "53741e28-c33a-4e05-aace-423b4ae3980a", "a3533c0b-395a-4a75-85e7-0df3a731a16d", "0ea30919-6662-4108-8505-ee1e82a766d5", "53677c48-7ffb-4785-b517-9274559c858e", "ef893316-98d6-45dc-8370-2315412286c2", "5626016d-5e1b-437a-bd25-fd81d0868d7f", "c35c9179-effd-4d67-9704-1badc6b7e839", "c7b440e0-969f-4779-888c-6ca7a0beecf1", "93f46ebd-8b05-4edc-9d48-08ca0ee13e3a", "931232af-5cb7-403d-ae50-1d2210c7c2e8", "b75c8597-2c53-4ada-a4c0-5521742ffc97", "237bf8ee-0ae6-47be-95c1-7a0ffb616d0a", "a1b45761-1f1d-4978-acce-2b3d8b5c9ba8", "a7ef46ea-6728-4619-802e-c60095860ef4", "13ecd6e5-0433-45d9-897d-a0bddf2f877e", "ed9b788a-0a99-450b-89dd-c71989751a90", "a64b24c4-b20d-4b3b-bfc0-b263653d5277", "48f57fbd-c397-489a-a376-a5fad1f11ef1", "c730193a-15a4-4e08-825d-91a44d24336a", "450f447f-4f99-4972-a174-ccba575c59d4", "08fd9cf9-f3cf-4e01-a1d2-dbcc6a2806cf", "20eb92b6-3150-4a6d-bd2f-0b93eeca0907", "a840c0b3-a993-41fa-9e83-3ee94a2fc0f4", "1d239534-c512-47c1-9d12-6f02621d01be", "a4a93181-1f03-4e0b-95df-9bbf7fbb253a", "75bf6d65-62b9-4ec8-8c7d-4f50be1131de", "ac9a557c-615d-4ea1-80bd-584ddd5d822e", "54d71c9e-54cc-4c2d-85a5-8a9b6fcd99d1", "fb347928-389d-421e-815a-b6aa0f876fe5", "76c078b1-11f0-4d65-897f-6aa5c9571799", "ff232f78-0e24-44e8-814d-faa8408b2da6", "d728fe5c-54d0-4b25-b7ba-805d5511d276", "af070e03-5b5b-4fb3-a320-901e86a0acc9", "f7f82782-ecc9-4eb9-b7f1-e89ceab8a78e", "2794d913-a759-4185-8646-10c22eca0aa9", "900e5cf2-3910-4297-bc78-3c659c7bc501", "d455a585-5522-4541-b44a-03fda4645af0", "12e7b3d8-115c-4850-aac7-5cb519a4acc1", "0b8b6885-c04b-4cb2-a13d-e821baeccf59", "75eba786-ac25-4689-9f03-fb0536c816ca", "42bc6317-d512-4c47-8041-9f72e06f0010", "5aa4ff0a-0085-4051-b61a-59dff2de2a60", "63298601-608a-48ed-8a46-75804b8fce05", "29cae91b-ee03-478e-aa2e-29a634a3b171", "d2d3c4fd-a140-4907-9c6f-cbe9a0a04d64", "276f08bf-9358-40ba-9679-8dafdc3c7396", "b5ff630d-f46f-44ff-9c47-f82902535393", "7c7aa9a2-57f6-4218-bc5c-67fce9e4b7b2", "bffd4363-177e-4168-bcf0-f52d862e2a2a", "65dc7959-1ae0-4838-8158-bcf85c494736", "1bd426ba-ac65-4c9c-954a-d6523afc1d17", "4398cfc8-ecb4-47f5-9d2c-0d2b6a226064", "99f68935-e6c8-453e-ab40-6ed392c21d28", "0b4cd0dd-eb9e-4f99-aa59-8fcb2750c627", "3b0d5bef-ea68-45bb-b96e-e207aa4d0e4c", "8c1b92dc-4924-4787-adb8-56c58c9bc88d", "2f3d1916-0efd-47da-870d-405dfeac96a7", "b47c0864-527d-4906-9c44-7824b61ddfdc", "6e437c6f-d6f8-466b-8c9a-df69c521aa4f", "afd39d03-145a-485b-8447-bfb31d9ba20b", "527f9a6a-9153-4e97-ab1e-66e6a4de1520", "a8391d42-38de-4960-a2bc-b8b4f4b7d60c", "da53c4b5-3a0b-4fbb-8439-d8175f6ba0d0", "e58c881f-d8a9-43e6-81a6-58d8a31d3c5d", "0fa5a21b-f94a-45d8-8d6d-8503b32da814", "d76db1e6-2c50-46f6-86c2-18fc9336188c", "c2acfe88-01ad-4c41-980b-889b47a7fe18", "d8ba33bd-174a-4859-bab8-cc6cd5d93cd5", "29dd3a14-b0a6-4b91-b9e3-bb866055b292", "7185cdd1-55f8-41cb-a140-ceeeb71923be", "e6f146ee-8e16-42a2-814d-2229cb121fd3", "0750ff7b-fe2a-43a6-8117-48d532fcfbd1", "f13f0e21-5eb4-45ac-aaf8-b811e1e68d06", "1b27d1a5-8c2b-4eb1-ab84-c91521b0b309", "e5606670-b78d-42d9-be7f-ac207d2f0fed", "75a2efef-53a2-4b88-af41-ec0a0ee72acb", "02846de9-fc21-4c6c-b9e0-add9100573ab", "7b4c74f1-6eea-4fbb-9fb4-2e474203b03a", "f36e124b-0d55-4cf1-9330-5ceeea753fae", "168e0630-e65c-4d44-99c4-06f117e1b480", "33f73d38-fe83-4811-9b73-6035f8e7fa16", "b9ca16f6-fda9-49c3-8f75-d7c7af38e07d", "271ff35a-e57b-4dff-9752-ccd6f97d260a", "4adad782-f25d-46bb-b931-65380c4c910a", "8fdb77fb-a62f-497b-a463-fa1195075fef", "7616de3f-cee3-4836-8e0a-64b1551d2eb3", "1e67ad64-83c9-403d-9338-f5792bcfabf4", "321d3784-2c03-477a-b839-5a63761fc0c9", "57adae60-2d3a-480c-bca4-d19bd1f4e2c3", "d76db1e6-2c50-46f6-86c2-18fc9336188c", "5d139ac9-bff7-4a90-a05a-50ee858ad3ca", "960deb82-00db-4dbd-b567-85d5e850196b", "1c55fbf8-0cb9-4d5e-9d2f-312aad8c7c8d", "73f629f9-c59c-40be-acb2-6f97e39d7c8f", "b1f0de16-fef9-4625-9fb8-e0a46754e688", "a5d9c0e7-20e5-4e83-8076-42400715b8a9", "be2d5f6c-db5c-4e68-832b-c337e822df54", "99c3b745-03d9-43f1-a8e1-86b03a4b306e", "10ebd964-5c95-4c8e-af18-8d7975483fb7", "29e0c8f5-90c5-4be4-87a8-60beda8a675b", "c01f601e-d44f-4b79-a2ff-d4320d1088a3", "ca3068d0-3c86-4c8e-9d78-541af9d46f61", "bef26d51-e0c4-4694-a8b7-61a1ba58e026", "1db153a3-46f2-4055-8ab7-3e7a000fdc0d", "b9662f5b-2f32-487a-8c82-e35c1658e1df", "daf236a6-c949-49ad-97a6-f0a0496658b0", "863bf918-0ed2-4899-88f4-015b6e8477ef", "1133506a-e706-436a-b1c0-117425503636", "68f8933e-e46c-4903-a734-925a806240f7", "edac8f8d-180d-49de-b73b-2682dbb199b7", "ca13ba12-702b-4797-9c16-970d16cf425b", "0ed8ff5c-89e9-47e9-831e-55667139ed2c", "5802f3e3-c0fe-4e43-912e-0ceadc38cbc3", "5a57879e-0dd9-47f9-b2ae-4af1b7df15d7", "dcbf83c2-26f2-44b0-adf4-bd5379be5841", "5ea3fb8f-a9ed-4e09-ac7f-debd5d0d8296", "960deb82-00db-4dbd-b567-85d5e850196b", "fd878da8-f50c-47e2-ab74-41c0ad3e576c", "fd28f845-12fc-4821-8bc0-fb61eb879979", "122fafa6-cf02-4640-9a80-469a704a77cf", "7d0121f9-d8b9-4e6f-a2a0-6e8f24a86531", "a30c154a-0151-41f9-baf7-e2e18ed9f101", "986c209f-0d6e-48f8-b9ba-4c3a50c0b575", "168e0630-e65c-4d44-99c4-06f117e1b480", "0e79268c-493e-4265-9677-4b39b3d1b62f", "66ab210c-ec41-4fa8-ae0d-3a9da4c8a8df", "52445063-883a-437f-ba68-3ee06ce90cce", "07c372fc-2ffa-4109-b152-1623e905ea86", "5aa2bca9-30be-4719-969f-afe2c84d4431", "0b4cd0dd-eb9e-4f99-aa59-8fcb2750c627", "743f9f7d-4307-403c-bc8e-ce111539b128", "0a4e07e5-83d3-45f6-a411-589687fa94ce", "2b10cf9d-37d9-45d0-b114-fcec37c2146b", "55473622-eaf8-471d-8b2f-14daae66cdb8", "3b2e4f92-49f3-4d17-9ae6-16ccc46cec0c", "d7e9dc6e-4d35-4c35-8eb2-1f518361cbcf", "960deb82-00db-4dbd-b567-85d5e850196b", "73216145-8df0-4898-ac61-b57488189010", "8b701b19-ef48-41d0-baff-f93184e1830c", "1290166c-f007-4413-abc7-74b1bb3759dc", "876bb313-b312-4ef0-8d94-790d52d1f8e3", "9ec1170a-1346-41b2-b8cb-39af7a38cd43", "6171909c-5e7e-4bef-ac54-9d0aa280930e", "1cefc633-54ff-4ca5-ac72-cb2dec7a62ba", "02a9b51b-9fcd-4db3-975d-bf995147ee0f", "0c7c1f07-e14b-4ace-8d6b-ef66fbbad19a", "010b46c8-bb2a-485b-b141-93f4d1c9a3eb", "b6b0fc19-ed21-4a26-89e8-8300e3d66e68", "2f145898-5659-46ed-92e2-799748663ecb", "04488571-16e0-45cd-b8c3-7353e029abd7", "14b98712-e986-48b6-adf8-cea8b76ccc4c", "110c7877-2044-4c92-9257-d5a93976d70b", "1aa2bb63-1ffe-44db-a970-4f8da0cedb83", "0dfd4bf7-d8b5-41e0-8c2a-3d2b9f812dd8", "dd177fd8-9084-4052-9fac-08cb7ba4011c", "e1d11b45-59dd-4b27-8a43-be52e0ff877d", "2dbcdab2-bcbe-4da6-a9d1-2cb9812c150f", "1aa66a12-36f3-4e95-874f-9416a5da2b71", "1797f6ab-3c62-4fcd-9ff1-8b50beda3b2c", "1e67ad64-83c9-403d-9338-f5792bcfabf4", "0e8bd904-0b0a-4439-9dda-0dc2028aa7b7", "8aa7eae8-421f-4f10-9003-227ef8ac7017", "bb68dee0-129e-4510-83ad-0adc4bf105d5", "eb23dbab-f88c-4c99-a084-a5682f3c1268", "fc55d984-61d1-4010-9ff7-a3cdd68258bc", "6be96561-7d2f-4bbd-b745-9f5643664175", "97a8058c-dc45-477b-8685-b54ee9de90b6", "b5271780-dcb6-45b6-ade2-c225d8f26291", "e271eb52-6d9d-4080-91ec-bf809a537a63", "417ec3a0-d215-4aa0-88bf-006c970d0217", "eea44747-5c99-4f69-baae-7ed79f58190e", "04e5045d-f697-4cd9-9d07-da41efee1240", "8c5ace22-93dd-417a-ab32-24621a4b932f", "f9895522-cd32-4703-8e3c-98ffa84ca4f3", "be369de8-870b-4407-91dd-3b0e8358a67f", "7be87213-8359-470c-a6ac-50ad51b19b1f", "db00aade-a5b8-4358-861c-b29999aad329", "664bfaf9-dcc1-4863-95ee-087c345f7c97", "e512c85b-f370-4b19-9813-ff8d80bc1329", "2aafbbae-d0fd-437c-88f8-070a62b42ba1", "f7cfcead-26f7-4ca3-8928-5a83e383c485", "d6524d99-e840-4cd6-99b5-736f46fb1327", "d58d289e-dc10-401c-86fa-1a7323b1b6ba", "fd58826a-c37e-4ec0-9a06-6e9e51599e04", "93635523-fd20-4f0e-84da-897e51d9eca8", "94fd8765-5c70-4d25-bf27-ace06e7c9358", "bfc80632-1718-42d6-a9b3-8cd8cb6ed005", "93635523-fd20-4f0e-84da-897e51d9eca8", "a011e83b-88ab-4943-b9d1-1eab5103d992", "5701a726-66de-4416-acd0-ac60717854a7", "5ab6069b-55ba-4484-95ba-d5bd8b63f7b6", "e9239d57-6375-4fd1-b2c1-8b09c872b96c", "69445a65-d5ab-4cba-919b-f091d4bce2c4", "ded31b4f-702a-41bb-a188-c5a5aaf14444", "7725201b-3f34-485e-803b-bdc6345885b4", "23eb179c-35ed-4999-a02a-5a5695910658", "12351f2b-d269-4146-97d4-585cfe221649", "e6033480-1548-40db-9489-223d8183126f", "059a851e-9eb3-479e-9aa6-4acdb666ca03", "5155c753-248f-412a-9ed4-18523b23fd9b", "931232af-5cb7-403d-ae50-1d2210c7c2e8", "b9ca16f6-fda9-49c3-8f75-d7c7af38e07d"]}