{"titles": ["Deep metric learning using triplet network", "Train longer, generalize better: closing the generalization gap in large batch training of neural networks", "The implicit bias of gradient descent on separable data", "Scalable methods for 8-bit training of neural networks", "Norm matters: efficient and accurate normalization schemes in deep networks", "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "Fix your classifier: the marginal value of training the last weight layer", "Semi-supervised deep learning by metric embedding", "Bayesian gradient descent: Online variational Bayes learning with increased robustness to catastrophic forgetting and weight pruning", "Deep unsupervised learning through spatial contrasting", "ACIQ: analytical clipping for integer quantization of neural networks", "Augment Your Batch: Improving Generalization Through Instance Repetition", "The knowledge within: Methods for data-free model compression", "Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency", "Quantized back-propagation: Training binarized neural networks with quantized gradients", "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?", "On the Blindspots of Convolutional Networks", "Neural gradients are lognormally distributed: understanding sparse and quantized training", "Image difference based segmentation using recursive neural networks", "Infer2Train: leveraging inference for better training of deep networks"], "ids": ["47b7c89f-ed14-4b0a-9169-8429ba6bc067", "1a150ccf-d645-40e2-a5bb-3a013a8fde21", "4ef5a096-bbf1-4b25-a0a0-44354cf3ee42", "15ab23e6-fb79-41a8-97cb-e1e884be3089", "5674dd95-cf42-46eb-bf76-6499ca319589"]}