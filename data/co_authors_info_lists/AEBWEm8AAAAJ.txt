{"titles": ["Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or\u2212", "Binarized neural networks", "Quantized neural networks: Training neural networks with low precision weights and activations", "Simultaneous denoising, deconvolution, and demixing of calcium imaging data", "Train longer, generalize better: closing the generalization gap in large batch training of neural networks", "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "No bad local minima: Data independent training error guarantees for multilayer neural networks", "Memristor-based multilayer neural networks with online gradient descent training", "The implicit bias of gradient descent on separable data", "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "Simple, fast and accurate implementation of the diffusion approximation algorithm for stochastic ion channels with multiple states", "Training binary multilayer neural networks for image classification using expectation backpropagation", "A structured matrix factorization framework for large scale calcium imaging data analysis", "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis", "Efficient\" shotgun\" inference of neural connectivity from highly sub-sampled activity data", "Characterizing implicit bias in terms of optimization geometry", "Conductance-based neuron models and the slow dynamics of excitability", "A shotgun sampling solution for the common input problem in neural connectivity inference", "Multi-scale approaches for high-speed imaging and analysis of large neural populations", "Fix your classifier: the marginal value of training the last weight layer", "A fully analog memristor-based neural network with online gradient training", "History dependent dynamics in a generic model of ion channels-an analytic study", "Implicit bias of gradient descent on linear convolutional networks", "Convergence of gradient descent on separable data", "Norm matters: efficient and accurate normalization schemes in deep networks", "Fast constrained non-negative matrix factorization for whole-brain calcium imaging data", "Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate", "Diffusion approximation-based simulation of stochastic ion channels: which method to use?", "Scalable methods for 8-bit training of neural networks", "The neuronal response at extended timescales: long-term correlations without long-term memory", "Hebbian learning rules with memristors", "Neuronal spike generation mechanism as an oversampling, noise-shaping a-to-d converter", "The global optimization geometry of shallow linear neural networks", "Bayesian gradient descent: Online variational bayes learning with increased robustness to catastrophic forgetting and weight pruning", "C Lacefield C, W Yang, M Ahrens, R Bruno, TM Jessell, DS Peterka, R Yuste, L Paninski,\u201cSimultaneous denoising, deconvolution, and demixing of calcium imaging data", "ACIQ: Analytical Clipping for Integer Quantization of neural networks", "Bifurcation analysis of two coupled Jansen-Rit neural mass models", "The neuronal response at extended timescales: a linearized spiking input\u2013output relation", "Mean Field Bayes Backpropagation: scalable training of multilayer neural networks with binary weights", "An exact reduction of the master equation to a strictly stable system with an explicit expression for the stationary distribution", "Augment your batch: better training with larger batches", "Quantized back-propagation: Training binarized neural networks with quantized gradients", "The neuron\u2019s response at extended timescales", "How do infinite width bounded norm networks look in function space?", "Seizure pathways: A model-based investigation", "On the Blindspots of Convolutional Networks", "Quantized neural network training and inference", "Analog multiplier using a memristive device and method for implemening Hebbian learning rules using memrisor arrays", "Memristor-based multilayer neural networks with online gradient descent training Supplementary Material-Appendix", "Analog multiplier using a memristive device and method for implemening hebbian learning rules using memrisor arrays", "A structured matrix factorization framework for large scale calcium imaging data analysis", "The slow dynamics of neuronal excitability", "Spiking input-output relation for general biophysical neuron models", "Slow dynamics of neuronal excitability under pulse stimulation", "The neuron as a population of ion channels: the emergence of a stochastic and history dependent mode", "Task Agnostic Continual Learning Using Online Variational Bayes", "Infer2Train: leveraging inference for better training of deep networks"], "ids": ["b888e00b-6b2d-4417-9dbe-bf2e51bf6f32", "e9222f62-7524-4a1c-b297-8e7c7f19ef49", "d98a8cb3-de9c-4ced-94bb-f14ffdb4a10f", "1a150ccf-d645-40e2-a5bb-3a013a8fde21", "dccf2a24-e87c-415f-9c08-084b43053e9c", "62cdb2b7-715a-4cf7-af4d-4ff309234cf2", "019232e5-a8f1-4275-875c-9efe0531735f", "4ef5a096-bbf1-4b25-a0a0-44354cf3ee42", "892980fd-0819-4004-945d-c48e1a61b82d", "4fe02e10-a05e-47e4-97f9-b484c7e5cc15", "a232ee4b-e438-4256-92f4-ecbe5955ea11", "798dd8a6-2939-48e3-a653-dfb0594197ee", "6eab302d-3eb1-4201-ab10-aaa52a182cfa", "0ae1c3e4-9071-440f-89e6-e5431a96e20c", "9f5a9d08-c0e7-4361-9d8b-c5c74121dcbf", "bacc8a10-91e5-4684-b1e5-d62a6b59f712", "f6fcc30b-cb5c-4fb3-aa30-23b9e995e2bc", "4cc10390-4801-4684-b44d-852734006559", "7cbff526-eff4-4fbd-acd6-c9cb1af04a27", "5b5369fb-530d-4c2b-9b81-64064a61cf2c", "0d828814-8f73-4392-87bb-3f7a8c9df8a9", "9b38d64b-1fba-4f0a-9f34-0453743f7205", "9b38d64b-1fba-4f0a-9f34-0453743f7205", "a232ee4b-e438-4256-92f4-ecbe5955ea11", "3a53f3cd-545d-489d-bb6e-13e76e64fad6", "2b0a9086-7a55-4fe2-b3ad-734b749c4031", "cc7b0064-3968-45a0-83ec-14f160eeacfb"]}