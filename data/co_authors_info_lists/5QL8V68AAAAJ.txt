{"titles": ["Pipelayer: A pipelined reram-based accelerator for deep learning", "C", "DudeTM: Building durable transactions with decoupling for persistent memory", "Sc-dcnn: Highly-scalable deep convolutional neural network using stochastic computing", "GraphR: Accelerating graph processing using ReRAM", "GraphP: Reducing communication for PIM-based graph processing with efficient data partition", "Exploring the hidden dimension in graph processing", "Admm-nn: An algorithm-hardware co-design framework of dnns using alternating direction methods of multipliers", "PermDNN: Efficient compressed DNN architecture with permuted diagonal matrices", "Squeezing out all the value of loaded data: An out-of-core graph processing system with reduced disk i/o", "Scalablebulk: Scalable cache coherence for atomic blocks in a lazy environment", "Wonderland: A novel abstraction-based out-of-core graph processing system", "VIBNN: Hardware acceleration of Bayesian neural networks", "Volition: scalable and precise sequential consistency violation detection", "E-RNN: Design optimization for efficient recurrent neural networks in FPGAs", "Hypar: Towards hybrid parallelism for deep learning accelerator array", "Towards ultra-high performance and energy efficiency of deep learning systems: an algorithm-hardware co-optimization framework", "OmniOrder: Directory-based conflict serialization of transactions", "HEIF: Highly efficient stochastic computing-based inference framework for deep neural networks", "Neu-NoC: A high-efficient interconnection network for accelerated neuromorphic systems", "Datasize-aware high dimensional configurations auto-tuning of in-memory cluster computing", "Reram-based accelerator for deep learning", "Rainbow: Efficient memory dependence recording with high replay parallelism for relaxed memory model", "Graphq: Scalable pim-based graph processing", "Hop: Heterogeneity-aware decentralized training", "BulkSMT: Designing SMT processors for atomic-block execution", "TIE: energy-efficient tensor train-based inference engine for deep neural network", "Patdnn: Achieving real-time DNN execution on mobile devices with pattern-based weight pruning", "A stochastic-computing based deep learning framework using adiabatic quantum-flux-parametron superconducting technology", "A hybrid framework for fast and accurate gpu performance estimation through source-level analysis and trace-based simulation", "Speedybox: Low-latency nfv service chains with cross-nf runtime consolidation", "PIMSim: A flexible and detailed processing-in-memory simulator", "BulkCommit: scalable and fast commit of atomic blocks in a lazy multiprocessor environment", "Power efficient sharing-aware gpu data management", "Non-structured DNN weight pruning considered harmful", "Counterminer: Mining big performance data from hardware counters", "G-tsc: Timestamp based coherence for gpus", "SReplay: Deterministic sub-group replay for one-sided communication", "OPR: deterministic group replay for one-sided communication", "Pacifier: Record and replay for relaxed-consistency multiprocessors with distributed directory protocol", "Capuchin: Tensor-based GPU Memory Management for Deep Learning", ": A Disk I/O Focused Parallel Out-of-Core Graph Processing System", "A Lightweight Isolation Mechanism for Secure Branch Predictors", "DNNGuard: An Elastic Heterogeneous DNN Accelerator Architecture against Adversarial Attacks", "AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators", "ReBNN: in-situ acceleration of binarized neural networks in ReRAM using complementary resistive cell", "Heterogeneity-Aware Asynchronous Decentralized Training", "iCELIA: A Full-Stack Framework for STT-MRAM-Based Deep Learning Acceleration", "plock: A fast lock for architectures with explicit inter-core message passing", "Improving multiprocessor performance with fine-grain coherence bypass", "Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training", "AsymNVM: An Efficient Framework for Implementing Persistent Data Structures on Asymmetric NVM Architecture", "TPShare: a time-space sharing scheduling abstraction for shared cloud via vertical labels", "EUI-64 Considered Harmful", "CSE: Parallel finite state machines with convergence set enumeration", "ReGraph: A Graph Processing Framework that Alternately Shrinks and Repartitions the Graph", "D", "Circuit implementation of floating point range reduction for trigonometric functions", "Optimized register renaming scheme for stack-based x86 operations", "TUPIM: A Transparent and Universal Processing-in-Memory Architecture for Unmodified Binaries", "DwarvesGraph: A High-Performance Graph Mining System with Pattern Decomposition", "A Fast Lock for Explicit Message Passing Architectures", "Guest Editors\u2019 Introduction to the Special Issue on Machine Learning Architectures and Accelerators", "ReversiSpec: Reversible Coherence Protocol for Defending Transient Attacks", "SympleGraph: distributed graph processing with precise loop-carried dependency guarantee", "3-D Partitioning for Large-scale Graph Processing", "AccQOC: Accelerating Quantum Optimal Control Based Pulse Generation", "A Comprehensive Evaluation of RDMA-enabled Concurrency Control Protocols", "Efficient Performance Estimation and Work-Group Size Pruning for OpenCL Kernels on GPUs", "Non-Structured DNN Weight Pruning\u2013Is It Beneficial in Any Platform?", "A Case for Asymmetric Non-Volatile Memory Architecture", "vS", "An Efficient Framework for Implementing Persist Data Structures on Remote NVM.", "OPR", "Scalable and flexible bulk architecture", "Self-modifying Code Implementation on Godson-X", "Design and Implementation of Floating Point Stack on General RISC Architecture", "A NOVEL ABSTRACTION-BASED OUT-OF-CORE GRAPH PROCESSING SYSTEM", "ScalableBulk: Scalable Cache Coherence for", "Alaa Alameldeen, Intel Mahdi Nazm Bojnordi, University of Utah Anirudh Badam, Microsoft Pradip Bose, IBM Research", "Industrial Session Chair", "MICRO 2018", "ISCA 2018 External Reviewers", "EE/CSCI 451: Parallel and Distributed Computation", "HPCA 2018 Program Committee", "OPR: Partial Deterministic Record and Replay for One-Sided Communication", "1 Motivation: Parallel Programming Productivity", "Formal Verification of BulkSC"], "ids": ["f2156b12-5b7f-42eb-b38a-d55f972a3f17", "4c0b1d13-f641-4d35-95e9-ddb58adafcf3", "9ee3e5a6-fb8e-41f4-93fc-7296d2682955", "162ce59a-2bee-40ff-bd52-bdbe95ef393e", "0ff732d8-983c-4c30-8e2a-626dcab1d13a", "3a079d93-084d-4215-a0cd-2047f27034a2", "db3d8fd2-704d-4b63-a760-57f109b85bfa", "362e5dfc-807d-4978-8fc5-ed46779f4f90", "fa0c6085-cc5c-4fb4-ab3e-a35253d51c32", "eea965b0-0a9c-41c6-b7fe-8c823497248c", "6f856dc5-a2df-4e79-9fb9-0d2c8a55e65f", "040a80a1-5912-4642-ab48-4a5bd3c31d3c", "f40f96fa-0513-4a94-b851-8cfd9eec4d17", "647c5935-496a-41b3-bf2b-69da43474020", "4419d8d1-838b-4995-8853-6b5dc546827d", "9e819d79-4535-416f-baf0-2a3211effdeb", "8f953874-64dc-4518-b019-1f82179ab725", "67248eac-deca-45ef-ae52-b03ea731080a"]}