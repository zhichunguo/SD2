{"titles": ["Designing topology-aware collective communication algorithms for large scale infiniband clusters: Case studies with scatter and gather", "Design of a scalable InfiniBand topology service to enable network-topology-aware placement of processes", "SR-IOV support for virtualization on infiniband clusters: Early experience", "Scalable memcached design for infiniband clusters using hybrid transports", "High-performance and scalable non-blocking all-to-all with collective offload on InfiniBand clusters: a study with parallel 3D FFT", "Designing power-aware collective communication algorithms for InfiniBand clusters", "MVAPICH-PRISM: A proxy-based communication framework using InfiniBand and SCIF for Intel MIC clusters", "Design and evaluation of network topology-/speed-aware broadcast algorithms for infiniband clusters", "Designing multi-leader-based allgather algorithms for multi-core clusters", "Efficient intra-node communication on intel-mic clusters", "Supporting hybrid MPI and OpenSHMEM over InfiniBand: Design and performance evaluation", "Designing non-blocking allreduce with collective offload on InfiniBand clusters: A case study with conjugate gradient solvers", "Designing optimized mpi broadcast and allreduce for many integrated core (mic) infiniband clusters", "Design and evaluation of generalized collective communication primitives with overlap using connectx-2 offload engine", "Designing non-blocking broadcast with collective offload on infiniband clusters: A case study with hpl", "MPI alltoall personalized exchange on GPGPU clusters: Design alternatives and benefit", "WOMBAT: A scalable and high-performance astrophysical magnetohydrodynamics code", "Evaluation of energy characteristics of mpi communication primitives with rapl", "MIC-RO: enabling efficient remote offload on heterogeneous many integrated core (MIC) clusters with InfiniBand", "Optimizing Cray MPI and SHMEM software stacks for Cray-XC supercomputers based on Intel KNL processors", "Design of network topology aware scheduling services for large infiniband clusters", "Evaluating the networking characteristics of the Cray XC\u201040 Intel Knights Landing\u2010based Cori supercomputer at NERSC", "Initial study of multi-endpoint runtime for MPI+ OpenMP hybrid programming model on multi-core systems", "A novel functional partitioning approach to design high-performance mpi-3 non-blocking alltoallv collective on multi-core systems", "Optimizing collective communication in openshmem", "Can network-offload based non-blocking neighborhood MPI collectives improve communication overheads of irregular graph algorithms?", "Efficient intranode desgins for openshmem on multicore clusters", "High performance design and implementation of nemesis communication layer for two-sided and one-sided mpi semantics in mvapich2", "INAM-a scalable infiniband network analysis and monitoring tool", "Co-designing MPI library and applications for infiniband clusters", "Symmetric memory partitions in OpenSHMEM: a case study with Intel KNL", "GPCNeT: designing a benchmark suite for inducing and measuring contention in HPC networks", "Current state of the cray MPT software stacks on the cray XC series supercomputers", "Optimizing Cray MPI and Cray SHMEM for Current and Next Generation Cray-XC Supercomputers", "Designing topology-aware communication schedules for alltoall operations in large infiniband clusters", "Designing Topology-Aware Collective Communication Algorithms for Large Scale InfiniBand Clusters: Case Studies wih Scatter and Gather", "High performance scalable deep learning with the cray programming environments deep learning plugin", "Designing network failover and recovery in MPI for multi-rail InfiniBand clusters", "InfiniBand Network Analysis and Monitoring using OpenSM", "Improving Parallel 3D FFT Performance using Hardware Offloaded Collective Communication on Modern InfiniBand Clusters", "Performance evaluation of MPI on Cray XC40 Xeon phi systems", "High Performance Non-Blocking Collective Communication for Next Generation Infiniband Clusters", "A Portable InfiniBand Module for MPICH2/Nemesis: Design and Evaluation", "GPCNeT", "Optimized MPI Gather Collective for Many Integrated Core (MIC) InfiniBand Clusters", "Codesign for InfiniBand Clusters-Codesigning applications and communication libraries to leverage underlying network features is imperative for achieving optimal performance on\u00a0\u2026", "Codesign for InfiniBand Clusters: Codesign for Systems and Applications: Charting the Path to Exascale Computing", "Collective Communication, Network Support For.", "Designing Multi-Core Aware Inter-Communicator Operations for MPI-2 Dynamic Process Management", "Cluster 2016 External Reviewers", "PADS 2012 Program Co-chairs", "An Enhanced MPI-2 Dynamic Process Management Support for InfiniBand", "Can Streaming SIMD Non-Temporal Instructions Benefit Intra-node MPI Communication on Modern Multi-core Platforms?"], "ids": ["2fbc3798-8c15-4d21-83a7-cedb19766d28", "656d1cc9-34ae-4110-b325-cd511e409b06", "d2f481ec-cda6-4129-96b7-d7bead02ecf1", "e0fbc405-2934-4437-b540-df8b78215f45", "f89fa085-18bd-4c03-afbc-d5d0d322e295", "87f009a1-984f-4151-9983-da9ca3177a0a", "b7cbb3a1-3e99-438e-9d3f-e38b4c5c9567", "933b1505-60d2-449e-80b9-8b6eac262d71", "02874e5a-87e9-448d-a4af-7720857bb0a3", "219c24b9-c243-465e-90ce-14f6ded1eed2", "40d7ae86-afed-44ef-bde0-f378b0b30d7f", "99747d2a-c813-4e5a-800a-e4a10b2b7efd", "a0075676-752e-4f54-8fd6-e7a2159c0ed7", "bf928808-c60e-4395-8d09-3a57e50d9f48", "32cf4806-641d-4193-98e3-0f971d130fcc", "d4063058-637d-4524-9034-2956360f77e4", "c3c96490-b9cb-41c2-b769-27e09014e521", "fa27d623-71a6-445d-b601-66a0c5f5aef3", "78bb4be3-4bc5-43e3-a0c9-c1197784a67e", "d1c78ab6-ec1f-44de-ad65-901f4c7db1b4", "5ccd965b-d499-4bf0-8a2e-5ce9fd98953f", "5dc35adf-a673-42de-bb61-7c5cf3a1673b", "09429b19-0b06-4230-b04a-e6c36c2ca430", "4dbba770-ed75-456f-b820-261bc9d37a46", "20e7a9f5-b9fa-4248-8540-d2f3c2a5c2f0", "47d8bdf3-8209-48af-a104-b9739db391a1"]}