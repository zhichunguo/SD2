{"titles": ["The Limitations of Deep Learning in Adversarial Settings", "Practical black-box attacks against machine learning", "Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks", "Ensemble adversarial training: Attacks and defenses", "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "Adversarial examples for malware detection", "cleverhans v1. 0.0: an adversarial machine learning library", "SoK: Towards the Science of Security and Privacy in Machine Learning", "On the (statistical) detection of adversarial examples", "Adversarial attacks on neural network policies", "Semi-supervised knowledge transfer for deep learning from private training data", "The space of transferable adversarial examples", "Crafting Adversarial Input Sequences for Recurrent Neural Networks", "Machine learning in adversarial settings", "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans", "Scalable Private Learning with PATE", "Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning", "Extending defensive distillation", "Making machine learning robust against adversarial inputs", "Technical report on the cleverhans v2. 1.0 adversarial examples library", "On the effectiveness of defensive distillation", "Security and science of agility", "On the protection of private information in machine learning systems: Two recent approches", "Detection under Privileged Information", "Adversarial vision challenge", "The challenge of verification and testing of machine learning", "The magazine archive includes every article published in Communications of the ACM for over the past 50 years.", "Characterizing the Limits and Defenses of Machine Learning in Adversarial Settings", "Privacy and machine learning: two unexpected allies?", "Adversarial Examples in Machine Learning", "Mapping sample scenarios to operational models", "On Evaluating Adversarial Robustness", "Analyzing and Improving Representations with the Soft Nearest Neighbor Loss", "A Marauder's Map of Security and Privacy in Machine Learning: An overview of current and future research directions for making machine learning secure and private", "Prototypical Examples in Deep Learning: Metrics, Characteristics, and Utility", "Machine Learning with Privacy by Knowledge Aggregation and Transfer", "On The Integrity Of Deep Learning Systems In Adversarial Settings", "Enforcing agile access control policies in relational databases using views", "2016 IEEE European Symposium on Security and Privacy (EuroS&P)(2016)", "1st Deep Learning and Security Workshop"], "ids": ["ec82cc0b-dc4b-4d61-bc96-5f30bbf7c059", "a8c50714-1656-418b-b10c-70eb46f2af5f", "491fcf02-c6fb-4f17-abac-02b9408c1f16", "dc174a08-ec87-45ee-b9cf-171705e91d3c", "f4f3bf01-b705-46c0-b9a4-cd88c7290d53", "fb549610-9491-47ab-8959-022c6ee725c5", "0f70d7e0-639c-4410-a51a-7b0bac427a64", "c1c7200c-b61d-4762-915a-b8d475f7325a", "5067d190-dc33-44c7-9fd1-936c9ac34e88", "ad03fb2f-fe09-4c71-ae6a-97e2ea7e9b3f", "d891c3ed-6cc5-4d09-a830-ccab05335d78", "310a1f94-eaa8-49d0-95dc-580e46e8c4c9", "9116921a-94ca-4129-9e68-2b5dfe1a1d1b", "03ee1194-4a70-4fbd-a40f-e5940c23b3f2", "39085d01-671f-477e-ad96-2ab6a1a97248", "67797b21-6b55-4ef6-9cde-9f00d470c19f"]}