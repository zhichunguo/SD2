{"titles": ["Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "Sequence to Sequence - Video to Text", "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images", "A Database for Fine Grained Activity Detection of Cooking Activities", "Neural module networks", "What helps where\u2013and why? semantic relatedness for knowledge transfer", "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "Learning to Compose Neural Networks for Question Answering", "Translating Video Content to Natural Language Descriptions", "Natural Language Object Retrieval", "A Dataset for Movie Description", "Generating Visual Explanations", "Grounding of Textual Phrases in Images by Reconstruction", "Learning to Reason: End-to-End Module Networks for Visual Question Answering", "Transfer Learning in a Transductive Setting", "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data", "Grounding Action Descriptions in Videos", "The Benefits of Dense Stereo for Pedestrian Detection", "Coherent Multi-Sentence Video Description with Variable Level of Detail", "Script Data for Attribute-based Recognition of Composite Activities", "Multi-view Pictorial Structures for 3D Human Pose Estimation", "The Long-Short Story of Movie Description", "Multimodal Explanations: Justifying Decisions and Pointing to the Evidence", "Modeling Relationships in Referential Expressions with Compositional Modular Networks", "Segmentation from Natural Language Expressions", "Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data", "Movie Description", "Multimodal Video Description", "A Multi-scale Multiple Instance Video Description Network", "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training", "Captioning Images with Diverse Objects", "High-Level Fusion of Depth and Intensity for Pedestrian Classification.", "Memory Aware Synapses: Learning what (not) to forget", "3D Object Detection with Multiple Kinects", "Commonsense in Parts: Mining Part-Whole Relations from the Web and Image Tags", "Combining language sources and robust semantic relatedness for attribute-based knowledge transfer", "Generating descriptions with grounded and co-referenced people", "Spatial Semantic Regularisation for Large Scale Object Detection", "Pythia v0.1: the Winning Entry to the VQA Challenge 2018", "Large-Scale Visual Relationship Understanding", "Visual Coreference Resolution in Visual Dialog using Neural Module Networks", "Utilizing Large Scale Vision and Text Datasets for Image Segmentation from Referring Expressions", "Selfless Sequential Learning", "Attributes as Semantic Units Between Natural Language and Visual Recognition", "Graph-Based Global Reasoning Networks", "Efficient Lifelong Learning with A-GEM", "A Dataset for Telling the Stories of Social Media Videos", "Combining visual recognition and computational linguistics: linguistic knowledge for visual recognition and natural language descriptions of visual content", "Combining Visual Recognition and Computational Linguistics", "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog", "Cycle-Consistency for Robust Visual Question Answering", "DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition", "Grounded Video Description", "Adversarial Inference for Multi-Sentence Video Description", "Continual Learning with Tiny Episodic Memories", "Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering", "Exploring the Challenges towards Lifelong Fact Learning", "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication", "Pythia-A platform for vision & language research", "Ask Your Neurons Again: Analysis of Deep Methods with Global Image Representation", "Relating Natural Language and Visual Recognition", "Visual Knowledge Transfer Using Semantic Relatedness", "Visual Knowledge Transfer Using Semantic Relatedness Measures: Visueller Wissentransfer Mit Hilfe Von Semantischen \u00c4hnlichkeitsma\u00dfen", "High-Level Fusion of Depth and Intensity for Pedestrian Classi\ufb01cation"], "ids": ["912a0f92-4dc7-4b5d-a4aa-3d52d6a623f1", "86fa87c2-572d-4847-9714-50f9a455010f", "556ecf97-59c0-4f1a-8c86-dbd3d891521f", "9b88deb9-b2d2-4cd3-b49e-d1abceae1ee3", "6cdbc5d1-e1d0-4675-84c6-6ed716b11d4f", "be7a875e-2e7f-4b7b-8558-69d3934a2063", "dc1e1e7b-dc80-4e5d-b606-6a0f29bff1d9", "cc947cde-4164-4c75-b132-10ce22854ebc", "3335943a-5b6a-4072-8294-a7434fb38ef7", "2a703991-1630-4b1e-bb84-62c47737cc57", "de60a2d8-eaf0-4caa-a9fe-c6ef0b879aa9", "dc48e36b-b498-4f6e-bc19-5208b00c44d6", "bc49fa45-4c12-4e7d-a913-bda270852946", "ba97031c-0482-4e43-a5f2-4229bfd10b35", "bad756bd-bc4c-401c-98d6-817fa1131ee4", "1906651b-5ef4-4354-ab1a-708890dd7bc8", "33b0615a-3550-4cec-99ba-6bbb90f366e1", "4d7d646b-dac0-4e2f-8e58-8ce2aa2ba6dd", "e808b6df-e0cb-4f6b-8a38-6a5472ca6777", "b3feba3c-a071-445e-a6f9-88923f69f7c4", "8d940f64-b412-4718-be65-395e4550802a", "059128dc-51aa-4811-8ec2-7baa6db4fc2a", "45a0fc5f-7ac9-4c3a-ae0d-e353a1373475", "000e9df3-d011-4b95-bb88-3e76586da6bd", "cc443bc9-0a41-42e2-8838-7d98c79cab63", "c179cbda-09c3-4803-99ee-ecd8812347a7", "80de85bc-94b7-427d-b7e7-0e4a62189115", "f1b18d0d-2c37-4e14-805e-491dccf448e6", "66ee7815-8e7b-4d41-b686-727ac40b0eca", "3a00707f-73fd-47a6-a3f3-c8e6935c6fe4", "d7a8d611-6735-4963-b1ac-caa76b443adc", "387d8baa-faa5-4f76-b6f5-05ddd53beb05", "fb637cca-6411-478f-b585-bb78d76eefed", "aedfca73-b536-4179-aa24-678c8d20868b", "2508bbe6-bc43-461c-adf4-dbba84d6fb16", "44f019d2-f9af-4ad9-84ca-f15c7dc6bcad", "12a58305-cace-4767-9cb3-d4ba44061501", "4c7b5b91-e30f-4ff3-989b-72431b03cd32"]}