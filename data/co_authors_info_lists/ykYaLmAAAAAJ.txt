{"titles": ["Memcached design on high performance rdma capable interconnects", "High performance RDMA-based design of HDFS over InfiniBand", "High-performance design of Hadoop RPC with RDMA over InfiniBand", "High-performance design of hbase with rdma over infiniband", "Designing topology-aware collective communication algorithms for large scale infiniband clusters: Case studies with scatter and gather", "High-performance RDMA-based design of Hadoop MapReduce over InfiniBand", "Design of a scalable InfiniBand topology service to enable network-topology-aware placement of processes", "Scalable memcached design for infiniband clusters using hybrid transports", "High-performance and scalable non-blocking all-to-all with collective offload on InfiniBand clusters: a study with parallel 3D FFT", "Performance analysis and evaluation of infiniband fdr and 40gige roce on hpc and cloud computing systems", "Rdma over ethernet\u2014a preliminary study", "MVAPICH-PRISM: A proxy-based communication framework using InfiniBand and SCIF for Intel MIC clusters", "Design and evaluation of network topology-/speed-aware broadcast algorithms for infiniband clusters", "Design and evaluation of benchmarks for financial applications using Advanced Message Queuing Protocol (AMQP) over InfiniBand", "Designing multi-leader-based allgather algorithms for multi-core clusters", "High performance data transfer in grid environment using gridftp over infiniband", "Extending openSHMEM for GPU computing", "Designing non-blocking allreduce with collective offload on InfiniBand clusters: A case study with conjugate gradient solvers", "Designing efficient FTP mechanisms for high performance data-transfer over InfiniBand", "Design and evaluation of generalized collective communication primitives with overlap using connectx-2 offload engine", "Optimized broadcast for deep learning workloads on dense-GPU InfiniBand clusters: MPI or NCCL?", "An in-depth performance characterization of CPU-and GPU-based DNN training on modern architectures", "Designing MPI library with dynamic connected transport (DCT) of InfiniBand: early experiences", "Adaptive and dynamic design for MPI tag matching", "Designing non-blocking broadcast with collective offload on infiniband clusters: A case study with hpl", "System-level scalable checkpoint-restart for petascale computing", "Improving application performance and predictability using multiple virtual lanes in modern multi-core infiniband clusters", "Streaming, low-latency communication in on-line trading systems", "Designing scalable out-of-core sorting with hybrid MPI+ PGAS programming models", "Designing next generation clusters: evaluation of InfiniBand DDR/QDR on Intel computing platforms", "Intra-socket and inter-socket communication in multi-core systems", "Performance of hpc middleware over infiniband wan", "MIC-RO: enabling efficient remote offload on heterogeneous many integrated core (MIC) clusters with InfiniBand", "Scalable distributed dnn training using tensorflow and cuda-aware mpi: Characterization, designs, and performance evaluation", "OC-DNN: Exploiting advanced unified memory capabilities in CUDA 9 and volta GPUs for out-of-core DNN training", "Scalable reduction collectives with data partitioning-based multi-leader design", "Contention-aware kernel-assisted mpi collectives for multi-/many-core systems", "Designing MPI library with on-demand paging (ODP) of infiniband: challenges and benefits", "Design of network topology aware scheduling services for large infiniband clusters", "ER", "Efficient and scalable multi-source streaming broadcast on gpu clusters for deep learning", "Designing Dynamic and Adaptive MPI Point-to-Point Communication Protocols for Efficient Overlap of Computation and Communication", "High performance MPI datatype support with user-mode memory registration: Challenges, designs, and benefits", "A high performance broadcast design with hardware multicast and GPUDirect RDMA for streaming applications on Infiniband clusters", "Pmi extensions for scalable mpi startup", "Wide-area overlay networking to manage science DMZ accelerated flows", "MPI performance engineering with the MPI tool interface: the integration of MVAPICH and TAU", "Designing efficient shared address space reduction collectives for multi-/many-cores", "INAM", "Impact of HPC Cloud Networking Technologies on Accelerating Hadoop RPC and HBase", "Exploiting GPUDirect RDMA in designing high performance OpenSHMEM for NVIDIA GPU clusters", "Non-blocking PMI extensions for fast MPI startup", "A novel functional partitioning approach to design high-performance mpi-3 non-blocking alltoallv collective on multi-core systems", "Efficient asynchronous communication progress for MPI without dedicated resources", "Designing non-blocking personalized collectives with near perfect overlap for rdma-enabled clusters", "Can network-offload based non-blocking neighborhood MPI collectives improve communication overheads of irregular graph algorithms?", "High performance design and implementation of nemesis communication layer for two-sided and one-sided mpi semantics in mvapich2", "Minimizing network contention in infiniband clusters with a qos-aware data-staging framework", "INAM-a scalable infiniband network analysis and monitoring tool", "Salar: Scalable and adaptive designs for large message reduction collectives", "CUDA-Aware OpenSHMEM: Extensions and Designs for High Performance OpenSHMEM on GPU Clusters", "A scalable InfiniBand network topology-aware performance analysis tool for MPI", "Understanding the communication characteristics in HBase: What are the fundamental bottlenecks?", "Co-designing MPI library and applications for infiniband clusters", "Co-designing MPI library and applications for infiniband clusters", "FALCON: Efficient Designs for Zero-Copy MPI Datatype Processing on Emerging Architectures", "Characterizing CUDA Unified Memory (UM)-Aware MPI Designs on Modern GPU Architectures", "Exploiting hardware multicast and GPUDirect RDMA for efficient broadcast", "Exploiting maximal overlap for non-contiguous data movement processing on modern gpu-enabled systems", "Designing high performance heterogeneous broadcast for streaming applications on GPU clusters", "SHMEMPMI--Shared Memory Based PMI for Improved Performance and Scalability", "Performance evaluation of MPI libraries on GPU-enabled OpenPOWER architectures: Early experiences", "Cooperative rendezvous protocols for improved performance and overlap", "Designing topology-aware communication schedules for alltoall operations in large infiniband clusters", "Designing Topology-Aware Collective Communication Algorithms for Large Scale InfiniBand Clusters: Case Studies wih Scatter and Gather", "Designing a Profiling and Visualization Tool for Scalable and In-depth Analysis of High-Performance GPU Clusters", "Communication Profiling and Characterization of Deep-Learning Workloads on Clusters With High-Performance Interconnects", "Performance characterization of dnn training using tensorflow and pytorch on modern clusters", "Efficient design for MPI asynchronous progress without dedicated resources", "Efficient reliability support for hardware multicast-based broadcast in GPU-enabled streaming applications", "On-demand connection management for OpenSHMEM and OpenSHMEM+ MPI", "Topology-Aware MPI Communication and Scheduling for High Performance Computing Systems", "Designing network failover and recovery in MPI for multi-rail InfiniBand clusters", "Design and characterization of infiniband hardware tag matching in MPI", "Scaling TensorFlow, PyTorch, and MXNet using MVAPICH2 for High-Performance Deep Learning on Frontera", "HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN Training using TensorFlow", "Designing Scalable and High-Performance MPI Libraries on Amazon Elastic Fabric Adapter", "Optimized large-message broadcast for deep learning workloads: MPI, MPI+ NCCL, or NCCL2?", "High performance distributed deep learning: a beginner's guide", "Networking and communication challenges for post-exascale systems", "A scalable network-based performance analysis tool for MPI on large-scale HPC systems", "GPU-Aware Design, Implementation, and Evaluation of Non-blocking Collective Benchmarks", "Improving Parallel 3D FFT Performance using Hardware Offloaded Collective Communication on Modern InfiniBand Clusters", "Designing QoS Aware MPI for InfiniBand", "NV-group: link-efficient reduction for distributed deep learning on modern dense GPU systems", "Communication-Aware Hardware-Assisted MPI Overlap Engine", "High-Performance Adaptive MPI Derived Datatype Communication for Modern Multi-GPU Systems", "Design and Characterization of Shared Address Space MPI Collectives on Modern Architectures", "Multi-Threading and Lock-Free MPI RMA Based Graph Processing on KNL and POWER Architectures", "Designing Registration Caching Free High-Performance MPI Library with Implicit On-Demand Paging (ODP) of InfiniBand", "Kernel-Assisted Communication Engine for MPI on Emerging Manycore Processors", "Offloaded GPU collectives using CORE-direct and CUDA capabilities on infiniband clusters", "Impact of InfiniBand DC transport protocol on energy consumption of all-to-all collective algorithms", "Implementing matrix multiplication on the Cell BE", "A Portable InfiniBand Module for MPICH2/Nemesis: Design and Evaluation", "Designing Zero-Copy FTP Mechanisms to Achieve High Performance Data-Transfer over InfiniBand WAN", "The MVAPICH Project: Transforming Research into High-Performance MPI Library for HPC Community", "Accelerated Real-time Network Monitoring and Profiling at Scale using OSU INAM", "HyPar-Flow: Exploiting MPI and Keras for Scalable ", "FALCON-X: Zero-copy MPI derived datatype processing on modern CPU and GPU architectures", "Analyzing and Understanding the Impact of Interconnect Performance on HPC, Big Data, and Deep Learning Applications: A Case Study with InfiniBand EDR and HDR", "Efficient Training of Semantic Image Segmentation on Summit using Horovod and MVAPICH2-GDR", "Performance Characterization of Network Mechanisms for Non-Contiguous Data Transfers in MPI", "Machine-agnostic and Communication-aware Designs for MPI on Emerging Architectures", "Leveraging Network-level parallelism with Multiple Process-Endpoints for MPI Broadcast", "Design and Evaluation of Shared Memory CommunicationBenchmarks on Emerging Architectures using MVAPICH2", "OMB-UM: Design, Implementation, and Evaluation of CUDA Unified Memory Aware MPI Benchmarks", "Unified and Federated Storage Middleware for Unstructured Big Data Analytics and Management", "4th International Workshop on Communication Architectures for HPC, Big Data, Deep Learning and Clouds at Extreme Scale (ExaComm 2018)", "PROCEEDINGS-IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING, ICCC", "\u8d85\u767e\u4ebf\u4ebf\u7ea7\u7cfb\u7edf\u9762\u4e34\u7684\u7f51\u7edc\u548c\u901a\u4fe1\u6311\u6218 (\u82f1\u6587)", "Exploiting and Evaluating OpenSHMEM on KNL Architecture", "IEEE IPDPS HPDIC 2013 Program Details", "Codesign for InfiniBand Clusters-Codesigning applications and communication libraries to leverage underlying network features is imperative for achieving optimal performance on\u00a0\u2026", "Codesign for InfiniBand Clusters: Codesign for Systems and Applications: Charting the Path to Exascale Computing", "Collective Communication, Network Support For.", "High Performance Topology-Aware Communication in Multicore Processors.", "COMPUTER ARCHITECTURE LETTERS", "Exploiting HPC for Distributed Deep Learning", "IPDPS 2020", "Leveraging Network-level parallelism with Multiple Process-Endpoints for MPI", "Theme Article: Hot Interconnects 26", "Yin, Junqi 84 Zhang, Zhao 45, 69", "Job Startup at Exascale", "IPDPS 2018 Outside Reviewers", "HPCC 2017", "Palchaudhuri, Ayan 104 Panda, Dhabaleswar K.(DK) 84, 213, 62 Panyala, Ajay 23 Park, Yoonho 94", "CLUSTER 2017", "Welcome to ESPM2'16 workshop! As the HPC field is heading to Exascale, the role of Programming Models and Middleware is getting more important. The objectives of this workshop\u00a0\u2026", "CLUSTER 2016", "Designing QoS Aware MPI for InfiniBand-Techinical Report", "A Report on Methods to Achieve High Performance Data Transfer through FTP", "A Report on Handling Delays Caused due to Node Mobiity in Wireless Networks"], "ids": ["333fdd26-fd85-492b-8902-ac01d62046e0", "f36c502b-2963-40a2-9459-dfe39364accf", "0a464dce-2319-490a-acb4-43e38230f938", "52922ef6-57e0-4d05-8431-9fc713ac49d7", "2fbc3798-8c15-4d21-83a7-cedb19766d28", "00afb4fb-1c3d-45be-aa27-6212821d08db", "656d1cc9-34ae-4110-b325-cd511e409b06", "e0fbc405-2934-4437-b540-df8b78215f45", "f89fa085-18bd-4c03-afbc-d5d0d322e295", "06c6c02a-8f2c-4574-896b-9e6b679f2fe5", "b7cbb3a1-3e99-438e-9d3f-e38b4c5c9567", "26e185c4-bf01-41c3-9338-e9e4cafd0970", "933b1505-60d2-449e-80b9-8b6eac262d71", "102e8d24-fb7f-4364-986d-43f4d441fdee", "567b01cb-9c0b-4333-86c7-ebe8f63600a1", "40d7ae86-afed-44ef-bde0-f378b0b30d7f", "c7d046ac-7757-4278-9df4-c7be9b0f2854", "a0075676-752e-4f54-8fd6-e7a2159c0ed7", "d0d515f0-ff53-40d7-8fa6-eb5efe55a177", "bf928808-c60e-4395-8d09-3a57e50d9f48", "91525f1b-eef4-4946-8c26-9109b49bb1a8", "006c29aa-2560-4099-865a-9ad9f784bac6", "3c61708d-41f9-4453-8d6e-e8082158dc82", "bdb72653-bd30-470b-9678-f2a141933f56", "a3f15cfa-415e-4a30-8576-b698490cbcdd", "c61200b2-c0f2-421b-a098-eb551887e3fc", "fa27d623-71a6-445d-b601-66a0c5f5aef3", "e5f350e0-1ce8-4c74-9760-971ba4627cb3", "78bb4be3-4bc5-43e3-a0c9-c1197784a67e", "2218a34e-4b0d-4e14-beb9-61c61bc83a84", "e3ea950a-c4a1-4ddc-b594-f71e29ca7e45", "c157df17-66f1-497f-bc5b-c0d2bf24e49d", "8b8d1e3a-5238-4918-baa4-8d6c757d83fb", "4d7ff8a4-92db-4dd5-a345-9d5623e652f2", "0eea21e5-40c2-4cbe-ace0-803712c5f033", "b74a3717-1981-453a-8dd6-69aa8d60aef2", "52f9a575-6bba-4b13-a124-1c811e19301f", "d1c78ab6-ec1f-44de-ad65-901f4c7db1b4", "9aa4a060-298d-4b7a-b82a-a6a99575313f", "5ccd965b-d499-4bf0-8a2e-5ce9fd98953f", "5dc35adf-a673-42de-bb61-7c5cf3a1673b", "3421c7ea-f4da-4415-9a03-50977f7f49fe", "50686444-f55f-4d85-b6cf-53e7318cfa7a", "2663b9d1-1fb4-4cd3-b3c2-a4f7592f013c", "bfbcb4ba-5ca7-431f-a568-b740f5f8a86e", "0d48cd7e-03d9-42ac-8bcd-fbc1da607c6b", "33832644-c4d6-484b-9740-fb0b504651f1", "09429b19-0b06-4230-b04a-e6c36c2ca430", "71d7d66d-5c17-4eeb-bc83-b563f67e6a18", "d66653b2-405d-4ee0-84be-39aa76bda987", "4dbba770-ed75-456f-b820-261bc9d37a46", "fef3c2b4-8d7c-4030-b89a-6d69f43d0047", "8a37c8ea-125a-4aac-8725-c52c1fea009c", "110eb91d-d22e-4a08-9779-0b681a9708ff"]}